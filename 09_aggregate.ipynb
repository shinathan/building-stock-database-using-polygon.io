{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.1 Getting higher timeframe bars\n",
    "I am interested in the 5-minute and 1-day timeframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import remove_extended_hours\n",
    "from datetime import datetime\n",
    "from tickers import get_id\n",
    "import mplfinance as mpf\n",
    "import pandas as pd\n",
    "import os\n",
    "DATA_PATH = \"../data/polygon/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = os.listdir(DATA_PATH + f'processed/m1/')\n",
    "for index, file in enumerate(all_files):\n",
    "    ticker = file[:-19] # Remove date and parquet extension\n",
    "\n",
    "    bars = pd.read_parquet(DATA_PATH + f'processed/m1/{file}')\n",
    "\n",
    "    # In the backtester we will use the opening price of the next bar to trade. So 'tradeable' has to be based on the first 1-minute bar within each 5-minute interval for 5-minute bars. However your approach might be different. If you use closing prices, you should use the last 1-minute bar.\n",
    "    m5 = bars.resample('5Min').agg({'open': 'first', \n",
    "                                    'high': 'max', \n",
    "                                    'low': 'min',\n",
    "                                    'close': 'last',\n",
    "                                    'close_original': 'last',\n",
    "                                    'turnover': 'sum',\n",
    "                                    'tradeable': 'first'})\n",
    "    # Pandas aggregation also adds non-trading days and times back. These can be easily removed by removing the NA values, because the first/max/min/last of nothing is NA. Also booleans are converted into 0/1, which has to be converted back.\n",
    "    m5 = m5.dropna()\n",
    "    m5['tradeable'] = m5['tradeable'].astype('bool')     \n",
    "    m5.to_parquet(DATA_PATH + f\"processed/m5/{file}\", engine=\"fastparquet\", row_group_offsets=25000)\n",
    " \n",
    "    # For the daily, we only consider a bar untradeable if it has zero trade prices. So if all \"tradeable\" is False. Also we only consider regular trading hours.\n",
    "    bars = remove_extended_hours(bars)\n",
    "    d1 = bars.resample('D').agg({'open': 'first', \n",
    "                                 'high': 'max', \n",
    "                                 'low': 'min',\n",
    "                                 'close': 'last',\n",
    "                                 'close_original': 'last',\n",
    "                                 'turnover': 'sum',\n",
    "                                 'tradeable': 'max'})\n",
    "    d1 = d1.dropna()\n",
    "    d1['tradeable'] = d1['tradeable'].astype('bool') \n",
    "    d1.to_parquet(DATA_PATH + f\"processed/d1/{file}\", engine=\"fastparquet\", row_group_offsets=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The daily bars look good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = pd.read_parquet(DATA_PATH + f\"processed/d1/{get_id('SPY')}.parquet\")\n",
    "bars = bars[bars.index > datetime(2020, 1, 1)]\n",
    "mpf.plot(bars.head(100), type='candle', style='yahoo', title='SPY ETF (daily)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 5-minute bars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars = pd.read_parquet(DATA_PATH + f\"processed/m5/{get_id('SPY')}.parquet\")\n",
    "bars = bars[bars.index > datetime(2022, 1, 31, hour=16)]\n",
    "mpf.plot(bars.head(250), type='candle', style='yahoo', title='SPY ETF (5-min)', show_nontrading=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are a lot of stupid spikes in the data. These are because of late reported trades and make the data almost useless if uncleaned. However cleaning them is very cumbersome because you need to recreate the bars from tick data.\n",
    "\n",
    "I actually e-mailed Polygon.io about this issue. They said they would propose to remove the late trades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.2 Updates\n",
    "Rerun after updating END_DATE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algotrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
