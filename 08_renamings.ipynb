{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1 Merging ticker changes\n",
    "This is optional. If you want it ticker-centric or don't want to get a stockanalysis.com subscription, you can just skip this part.\n",
    "\n",
    "To get a list of ticker changes, We can loop through all tickers and query <code>Ticker Events</code> but this only works with non-delisted companies. And although you can infer it based on the ticker list by looking at whether the cik or figi has changed, that is very messy. Because a company can stay the same even if the ticker and cik/figi change. I actually did it, and it did found that it did not match the Polygon <code>Ticker Events</code>. Then I stumbled on [stockanalysis.com](https://stockanalysis.com/actions/changes/) where you can find all ticker changes for only 10 bucks a month. The first month is even free. You have to manually download them for each year and put them in the <code>stockanalysis/raw/ticker_changes/</code> folder.\n",
    "\n",
    "After merging those we will save the result to <code>raw/renamings.csv</code> which will contain the columns <code>['from', 'to', 'now', 'date']</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_tickers, get_market_dates, get_ticker_changes, get_market_calendar\n",
    "from datetime import datetime, date, time\n",
    "import mplfinance as mpf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "DATA_PATH = \"../data/polygon/\"\n",
    "END_DATE = date(2024, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Aggregate the csv's\n",
    "all_ticker_changes = []\n",
    "for file in os.listdir(DATA_PATH + \"../stockanalysis/raw/ticker_changes/\"):\n",
    "    ticker_changes_year = pd.read_csv(DATA_PATH + \"../stockanalysis/raw/ticker_changes/\" + file, parse_dates=True, index_col=0, usecols=[\"Date\", \"Old\", \"New\"])\n",
    "    all_ticker_changes.append(ticker_changes_year)\n",
    "\n",
    "ticker_changes = pd.concat(all_ticker_changes)\n",
    "ticker_changes = pd.concat(all_ticker_changes)\n",
    "ticker_changes.rename(columns={\"Old\": \"from\", \n",
    "                               \"New\": \"to\"}, inplace=True)\n",
    "ticker_changes.index.names = ['date']\n",
    "ticker_changes.sort_index(inplace=True)\n",
    "ticker_changes.to_csv(DATA_PATH + \"../stockanalysis/ticker_changes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4347\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-06-09</th>\n",
       "      <td>FB</td>\n",
       "      <td>META</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           from    to\n",
       "date                 \n",
       "2022-06-09   FB  META"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(ticker_changes))\n",
    "ticker_changes[ticker_changes['from'] == \"FB\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a whopping 4347 ticker changes from 2003 to now that we have to take care of. But at least this was very easy to get.\n",
    "\n",
    "Now we will use them to merge our data. We have to be aware that it is possible for a ticker to used multiple times, so the <code>ticker_changes.csv</code> may contain multiple of the same tickers in the 'from' and 'to' column. \n",
    "\n",
    "After processing the ticker changes we will create a <code>tickers_v5.csv</code> which will be our definitive ticker list. This contains a column 'tickers_old', which will containa list of (date_of_change, ticker) pairs. So if A changes to B on day 2, and B changes to C on day 5, tickers_old for D will contain [[2, A], [5, B]].\n",
    "\n",
    "The process will be as follows:\n",
    "* As long as we have ticker changes to process\n",
    "    * Loop through <code>tickers_v4.csv</code>.\n",
    "        * Get the next trading date after 'end_date_data'.\n",
    "        * Search in <code>tickers_changes.csv</code> if there is a ticker change on this date.\n",
    "        * If it does:\n",
    "            * The stock data will be merged.\n",
    "            * In <code>tickers_v4.csv</code> we will change \"ticker\" to the new ticker and add a list [date, ticker] to \"tickers_old\".\n",
    "            * All other rows will be merged such as \"start_date\". For identifiers such as FIGI we will take the last available value. For the ID we will keep the original. If we do not do this, we might run into problems with identical IDs.\n",
    "            * The row of the old ticker will be deleted\n",
    "            * **We need to restart the loop!** If we don't the following can happen: Let's assume that a ticker was renamed from A -> B -> C -> D but that the order in which it appears in our ticker list is C, D, A, B. Using our loop, C gets merged with D. Then the loop checks D, which has no renamings. Then A gets merged with B. Then B gets merged with C, however that is incorrect! B should be merged with the new D, which contains C. Any double+ renamings have the risk of being in the 'wrong order'.\n",
    "            * Of course we must not forget that there can be adjustments on day 1 of the ticker change. There should be laws to prohibit this.\n",
    "\n",
    "Note: if a ticker A goes OTC and then comes back and changes to B, then we will have two files: one of the A before OTC and the A+B after OTC named B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/polygon/processed/m1/AACQ-2020-09-04.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m tickers_v4\u001b[38;5;241m.\u001b[39mloc[index_to, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_data\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m start_data_from\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Do the actual merging\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m from_ \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprocessed/m1/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mid_from\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m to \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(DATA_PATH \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed/m1/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mid_to\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Because companies like to be annoying, a split/dividend can take place at the same time as a ticker change. We have to account for this. An example is TYDE -> OCTO with a 50:1 reverse split. However this is much easier than 5_process_raw_data.ipynb, because there is at most a single split. This is rare, but there are still a handful of cases.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\miniconda3\\envs\\algotrading\\Lib\\site-packages\\pandas\\io\\parquet.py:509\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, **kwargs)\u001b[0m\n\u001b[0;32m    506\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    507\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\miniconda3\\envs\\algotrading\\Lib\\site-packages\\pandas\\io\\parquet.py:220\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, use_nullable_dtypes, dtype_backend, storage_options, **kwargs)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    218\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m path_or_handle, handles, kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfilesystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    227\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[0;32m    228\u001b[0m         path_or_handle, columns\u001b[38;5;241m=\u001b[39mcolumns, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    229\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\miniconda3\\envs\\algotrading\\Lib\\site-packages\\pandas\\io\\parquet.py:110\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m    100\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\miniconda3\\envs\\algotrading\\Lib\\site-packages\\pandas\\io\\common.py:868\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    869\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    871\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/polygon/processed/m1/AACQ-2020-09-04.parquet'"
     ]
    }
   ],
   "source": [
    "tickers_v4 = get_tickers(v=4)\n",
    "market_dates = get_market_dates()\n",
    "ticker_changes = get_ticker_changes()\n",
    "\n",
    "tickers_v4.insert(loc = 2, column = 'tickers_old', value = [[] for _ in range(len(tickers_v4))])\n",
    "\n",
    "while True:\n",
    "    # tickers_v4 gets smaller by 1 element every time we run this loop.\n",
    "    for index_from, row_from in tickers_v4.copy().iterrows():\n",
    "        # Get values\n",
    "        type_from = row_from['type']\n",
    "        if type_from == \"INDEX\":\n",
    "            continue\n",
    "        id_from = row_from['ID']\n",
    "        ticker_from = row_from['ticker']\n",
    "        start_date_from = row_from['start_date']\n",
    "        end_date_from = row_from['end_date']\n",
    "        start_data_from = row_from['start_data']\n",
    "        end_data_from = row_from['end_data']\n",
    "        if end_data_from == END_DATE:\n",
    "            continue\n",
    "\n",
    "        start_data_to = market_dates[market_dates.index(end_data_from) + 1]\n",
    "\n",
    "        # Get ticker changes \n",
    "        change = ticker_changes[(ticker_changes.index == start_data_to) & (ticker_changes['from'] == ticker_from)]\n",
    "        if change.empty:\n",
    "            continue\n",
    "        elif len(change) > 1:\n",
    "            raise Exception(\"Duplicate!\")\n",
    "        ticker_to = change['to'].values[0]\n",
    "\n",
    "        # Set values of new ticker\n",
    "        row_to = tickers_v4[(tickers_v4['start_data'] == start_data_to) & (tickers_v4['ticker'] == ticker_to)]\n",
    "        if row_to.empty:\n",
    "            continue\n",
    "        index_to = row_to.index[0]\n",
    "        id_to = row_to['ID'].values[0]\n",
    "        tickers_v4.loc[index_to, \"tickers_old\"].append([start_data_to.isoformat(), ticker_from])\n",
    "        tickers_v4.loc[index_to, \"start_date\"] = start_date_from\n",
    "        tickers_v4.loc[index_to, \"start_data\"] = start_data_from\n",
    "\n",
    "        # Do the actual merging\n",
    "        from_ = pd.read_parquet(DATA_PATH + f\"processed/m1/{id_from}.parquet\")\n",
    "        to = pd.read_parquet(DATA_PATH + f\"processed/m1/{id_to}.parquet\")\n",
    "\n",
    "        # Because companies like to be annoying, a split/dividend can take place at the same time as a ticker change. We have to account for this. An example is TYDE -> OCTO with a 50:1 reverse split. However this is much easier than 5_process_raw_data.ipynb, because there is at most a single split. This is rare, but there are still a handful of cases.\n",
    "        if os.path.isfile(DATA_PATH + f\"raw/adjustments/{ticker_to}.csv\"):\n",
    "            adjustments = pd.read_csv(DATA_PATH + f\"raw/adjustments/{ticker_to}.csv\", parse_dates=True, index_col=0)\n",
    "            adjustments.index = pd.to_datetime(adjustments.index).date\n",
    "            adjustments = adjustments[(adjustments.index == start_data_to)]\n",
    "\n",
    "            # SPLIT ADJUSTMENT\n",
    "            split = adjustments[adjustments.type == 'SPLIT']\n",
    "            if len(split) > 0:\n",
    "                split_amount = split['amount'][0]\n",
    "\n",
    "                from_[['open', 'high', 'low', 'close']] = from_[['open', 'high', 'low', 'close']].multiply(split_amount)\n",
    "                from_['volume'] = from_['volume'].divide(split_amount)\n",
    "\n",
    "            # DIVIDEND ADJUSTMENT - REUN is the only case, not clear what happened there.\n",
    "            dividend = adjustments[adjustments.type == 'DIV']\n",
    "            if len(dividend) > 0:\n",
    "                print(ticker_to)\n",
    "                market_hours = get_market_calendar()\n",
    "                market_hours = market_hours[['regular_close']]\n",
    "\n",
    "                cum_div_date = end_data_from\n",
    "                cum_div_time = market_hours.loc[cum_div_date][0]\n",
    "                cum_div_datetime = datetime.combine(cum_div_date, cum_div_time)\n",
    "                cum_div_datetime = (from_[from_.index <= cum_div_datetime].index).max()\n",
    "                cum_div_close = from_.loc[cum_div_datetime, 'close']\n",
    "                dividend_amount = dividend['amount'][0]\n",
    "                    \n",
    "                adjustment_factor = 1 - dividend_amount/cum_div_close\n",
    "\n",
    "                from_[['open', 'high', 'low', 'close']] = from_[['open', 'high', 'low', 'close']].multiply(adjustment_factor)\n",
    "                from_['volume'] = from_['volume'].divide(adjustment_factor)\n",
    "                from_ = round(from_, 4)\n",
    "                from_['volume'] = from_['volume'].astype(int)\n",
    "            \n",
    "            # ROUNDING\n",
    "            if len(split) > 0 or len(dividend) > 0:\n",
    "                from_ = round(from_, 4)\n",
    "                from_['volume'] = from_['volume'].astype(int)\n",
    "\n",
    "        # If on the old ticker, there are divs/splits on start_data_to (start of new ticker), then something is wrong.\n",
    "        if os.path.isfile(DATA_PATH + f\"raw/adjustments/{ticker_from}.csv\"):\n",
    "            adjustments = pd.read_csv(DATA_PATH + f\"raw/adjustments/{ticker_from}.csv\", parse_dates=True, index_col=0)\n",
    "            adjustments.index = pd.to_datetime(adjustments.index).date\n",
    "            adjustments = adjustments[(adjustments.index == start_data_to)]\n",
    "            assert len(adjustments) == 0\n",
    "\n",
    "        pd.concat([from_, to]).to_parquet(DATA_PATH + f\"processed/m1/{id_to}.parquet\", engine=\"fastparquet\", row_group_offsets=25000)\n",
    "        # pd.concat([from_, to]).to_parquet(DATA_PATH + f\"processed/m1/{new_id}.parquet\", engine=\"pyarrow\", compression = 'brotli', row_group_size = 25000)\n",
    "\n",
    "        # Delete old ticker from ticker list. You can also remove the files, but that is not necessary. In backtesting you will always loop over the ticker list and not over de files in the folder.\n",
    "        #os.remove(path = DATA_PATH + f\"processed/m1/{id_from}.parquet\") # Removal of old renamed ticker (not necessary)\n",
    "        tickers_v4.drop(index_from, inplace=True)\n",
    "        tickers_v4.reset_index(inplace=True, drop=True)\n",
    "        print(f\"Ticker change {ticker_from} -> {ticker_to} on {start_data_to} has been processed\")\n",
    "        print(f\"{index_from/len(tickers_v4)*100:.1f}% | Length of tickers_v4 is {len(tickers_v4)}\")\n",
    "        break\n",
    "    \n",
    "    # If we have reached the end of the loop, it means we have processed everything. Then we can stop.\n",
    "    if index_from >= (len(tickers_v4)-1):\n",
    "        break\n",
    "\n",
    "tickers_v4.to_csv(\"../data/tickers_v5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>subtype</th>\n",
       "      <th>amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-10-01</th>\n",
       "      <td>DIV</td>\n",
       "      <td>SO</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           type subtype  amount\n",
       "2020-10-01  DIV      SO     1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjustments[adjustments.type == 'DIV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dividend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "747\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ticker</th>\n",
       "      <th>tickers_old</th>\n",
       "      <th>name</th>\n",
       "      <th>active</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>start_data</th>\n",
       "      <th>end_data</th>\n",
       "      <th>type</th>\n",
       "      <th>cik</th>\n",
       "      <th>composite_figi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4319</th>\n",
       "      <td>META-2019-01-01</td>\n",
       "      <td>META</td>\n",
       "      <td>[['2022-06-09', 'FB']]</td>\n",
       "      <td>Meta Platforms, Inc. Class A Common Stock</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2023-09-07</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>2023-09-07</td>\n",
       "      <td>CS</td>\n",
       "      <td>1326801.0</td>\n",
       "      <td>BBG000MM2P62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID ticker             tickers_old  \\\n",
       "4319  META-2019-01-01   META  [['2022-06-09', 'FB']]   \n",
       "\n",
       "                                           name  active  start_date  \\\n",
       "4319  Meta Platforms, Inc. Class A Common Stock    True  2019-01-01   \n",
       "\n",
       "        end_date  start_data    end_data type        cik composite_figi  \n",
       "4319  2023-09-07  2019-01-02  2023-09-07   CS  1326801.0   BBG000MM2P62  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers_v5 = get_tickers(v=5)\n",
    "renamings = tickers_v5[tickers_v5[\"tickers_old\"].str.len() > 2] # These were renamed\n",
    "print(len(renamings))\n",
    "tickers_v5[tickers_v5['ticker'] == 'META']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tickers that were renamed multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ticker</th>\n",
       "      <th>tickers_old</th>\n",
       "      <th>name</th>\n",
       "      <th>active</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>start_data</th>\n",
       "      <th>end_data</th>\n",
       "      <th>type</th>\n",
       "      <th>cik</th>\n",
       "      <th>composite_figi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2363</th>\n",
       "      <td>ERNA-2019-01-01</td>\n",
       "      <td>ERNA</td>\n",
       "      <td>[['2022-10-17', 'BTX'], ['2021-03-26', 'NTN']]</td>\n",
       "      <td>Eterna Therapeutics Inc. Common Stock</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2023-09-07</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>2023-09-07</td>\n",
       "      <td>CS</td>\n",
       "      <td>748592.0</td>\n",
       "      <td>BBG000CX5677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2691</th>\n",
       "      <td>FRGT-2019-01-01</td>\n",
       "      <td>FRGT</td>\n",
       "      <td>[['2022-05-27', 'HUSN'], ['2020-05-08', 'CIFS']]</td>\n",
       "      <td>Freight Technologies, Inc. Ordinary Shares</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2023-09-07</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>2023-09-07</td>\n",
       "      <td>CS</td>\n",
       "      <td>1687542.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID ticker  \\\n",
       "2363  ERNA-2019-01-01   ERNA   \n",
       "2691  FRGT-2019-01-01   FRGT   \n",
       "\n",
       "                                           tickers_old  \\\n",
       "2363    [['2022-10-17', 'BTX'], ['2021-03-26', 'NTN']]   \n",
       "2691  [['2022-05-27', 'HUSN'], ['2020-05-08', 'CIFS']]   \n",
       "\n",
       "                                            name  active  start_date  \\\n",
       "2363       Eterna Therapeutics Inc. Common Stock    True  2019-01-01   \n",
       "2691  Freight Technologies, Inc. Ordinary Shares    True  2019-01-01   \n",
       "\n",
       "        end_date  start_data    end_data type        cik composite_figi  \n",
       "2363  2023-09-07  2019-01-02  2023-09-07   CS   748592.0   BBG000CX5677  \n",
       "2691  2023-09-07  2019-01-02  2023-09-07   CS  1687542.0                 "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_renamings = tickers_v5[tickers_v5[\"tickers_old\"].str.len() > 30]\n",
    "print(len(multiple_renamings))\n",
    "multiple_renamings.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 5 tickers lists. These are:\n",
    "1. Basic ticker list with a lot of incorrect duplications.\n",
    "2. Duplications merged and incorrect tickers removed.\n",
    "3. ETFs added.\n",
    "4. Data start/end dates added.\n",
    "5. Renamings merged.\n",
    "Only the last should be used in backtesting.\n",
    "\n",
    "If Polygon just provided these from the start, it would have saved countless hours. But at least I learned some Python I guess. And at least Polygon does not ask thousands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.2 Updates\n",
    "Rerun the file after setting END_DATE and updating the list of renamings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algotrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
