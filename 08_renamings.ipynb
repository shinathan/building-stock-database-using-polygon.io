{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1 Merging ticker changes\n",
    "This is optional. If you want it ticker-centric or don't want to get a stockanalysis.com subscription, you can just skip this part.\n",
    "\n",
    "To get a list of ticker changes, We can loop through all tickers and query <code>Ticker Events</code> but this only works with non-delisted companies. And although you can infer it based on the ticker list by looking at whether the cik or figi has changed, that is very messy. Because a company can stay the same even if the ticker and cik/figi change. I actually did it, and it did found that it did not match the Polygon <code>Ticker Events</code>. Then I stumbled on [stockanalysis.com](https://stockanalysis.com/actions/changes/) where you can find all ticker changes for only 10 bucks a month. The first month is even free. You have to manually download them for each year and put them in the <code>stockanalysis/raw/ticker_changes/</code> folder.\n",
    "\n",
    "After merging those we will save the result to <code>raw/renamings.csv</code> which will contain the columns <code>['from', 'to', 'now', 'date']</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tickers import get_tickers, get_ticker_changes\n",
    "from times import get_market_dates, get_market_calendar\n",
    "from datetime import datetime, date, time\n",
    "import pandas as pd\n",
    "import os\n",
    "DATA_PATH = \"../data/polygon/\"\n",
    "END_DATE = date(2024, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Aggregate the csv's\n",
    "all_ticker_changes = []\n",
    "for file in os.listdir(DATA_PATH + \"../stockanalysis/raw/ticker_changes/\"):\n",
    "    ticker_changes_year = pd.read_csv(DATA_PATH + \"../stockanalysis/raw/ticker_changes/\" + file, parse_dates=True, index_col=0, usecols=[\"Date\", \"Old\", \"New\"])\n",
    "    all_ticker_changes.append(ticker_changes_year)\n",
    "\n",
    "ticker_changes = pd.concat(all_ticker_changes)\n",
    "ticker_changes = pd.concat(all_ticker_changes)\n",
    "ticker_changes.rename(columns={\"Old\": \"from\", \n",
    "                               \"New\": \"to\"}, inplace=True)\n",
    "ticker_changes.index.names = ['date']\n",
    "ticker_changes.sort_index(inplace=True)\n",
    "ticker_changes.to_csv(DATA_PATH + \"../stockanalysis/ticker_changes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4347\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4013</th>\n",
       "      <td>2022-06-09</td>\n",
       "      <td>FB</td>\n",
       "      <td>META</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date from    to\n",
       "4013  2022-06-09   FB  META"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_changes = pd.read_csv(DATA_PATH + \"../stockanalysis/ticker_changes.csv\")\n",
    "print(len(ticker_changes))\n",
    "ticker_changes[ticker_changes['from'] == \"FB\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a whopping 4347 ticker changes from 2003 to now that we have to take care of. But at least this was very easy to get.\n",
    "\n",
    "Now we will use them to merge our data. We have to be aware that it is possible for a ticker to used multiple times, so the <code>ticker_changes.csv</code> may contain multiple of the same tickers in the 'from' and 'to' column. \n",
    "\n",
    "After processing the ticker changes we will create a <code>tickers_v5.csv</code> which will be our definitive ticker list. This contains a column 'tickers_old', which will containa list of (date_of_change, ticker) pairs. So if A changes to B on day 2, and B changes to C on day 5, tickers_old for D will contain [[2, A], [5, B]].\n",
    "\n",
    "The process will be as follows:\n",
    "* As long as we have ticker changes to process\n",
    "    * Loop through <code>tickers_v4.csv</code>.\n",
    "        * Get the next trading date after 'end_date_data'.\n",
    "        * Search in <code>tickers_changes.csv</code> if there is a ticker change on this date.\n",
    "        * If it does:\n",
    "            * The stock data will be merged.\n",
    "            * In <code>tickers_v4.csv</code> we will change \"ticker\" to the new ticker and add a list [date, ticker] to \"tickers_old\".\n",
    "            * All other rows will be merged such as \"start_date\". For identifiers such as FIGI we will take the last available value. For the ID we will keep the original. If we do not do this, we might run into problems with identical IDs.\n",
    "            * The row of the old ticker will be deleted\n",
    "            * **We need to restart the loop!** If we don't the following can happen: Let's assume that a ticker was renamed from A -> B -> C -> D but that the order in which it appears in our ticker list is C, D, A, B. Using our loop, C gets merged with D. Then the loop checks D, which has no renamings. Then A gets merged with B. Then B gets merged with C, however that is incorrect! B should be merged with the new D, which contains C. Any double+ renamings have the risk of being in the 'wrong order'.\n",
    "            * Of course we must not forget that there can be adjustments on day 1 of the ticker change. There should be laws to prohibit this.\n",
    "\n",
    "Note: if a ticker A goes OTC and then comes back and changes to B, then we will have two files: one of the A before OTC and the A+B after OTC named B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_v4 = get_tickers(v=4)\n",
    "market_dates = get_market_dates()\n",
    "ticker_changes = get_ticker_changes()\n",
    "\n",
    "tickers_v4.insert(loc = 2, column = 'tickers_old', value = [[] for _ in range(len(tickers_v4))])\n",
    "\n",
    "while True:\n",
    "    # tickers_v4 gets smaller by 1 element every time we run this loop.\n",
    "    for index_from, row_from in tickers_v4.copy().iterrows():\n",
    "        # Get values\n",
    "        type_from = row_from['type']\n",
    "        if type_from == \"INDEX\":\n",
    "            continue\n",
    "        id_from = row_from['ID']\n",
    "        ticker_from = row_from['ticker']\n",
    "        start_date_from = row_from['start_date']\n",
    "        end_date_from = row_from['end_date']\n",
    "        start_data_from = row_from['start_data']\n",
    "        end_data_from = row_from['end_data']\n",
    "\n",
    "        if end_data_from == END_DATE:\n",
    "            continue\n",
    "\n",
    "        start_data_to = market_dates[market_dates.index(end_data_from) + 1]\n",
    "\n",
    "        # Get ticker changes \n",
    "        change = ticker_changes[(ticker_changes.index == start_data_to) & (ticker_changes['from'] == ticker_from)]\n",
    "        if change.empty:\n",
    "            continue\n",
    "        elif len(change) > 1:\n",
    "            raise Exception(\"Duplicate!\")\n",
    "        ticker_to = change['to'].values[0]\n",
    "\n",
    "        # Set values of new ticker\n",
    "        row_to = tickers_v4[(tickers_v4['start_data'] == start_data_to) & (tickers_v4['ticker'] == ticker_to)]\n",
    "        if row_to.empty:\n",
    "            continue\n",
    "        index_to = row_to.index[0]\n",
    "        id_to = row_to['ID'].values[0]\n",
    "        tickers_v4.loc[index_to, \"tickers_old\"].append([start_data_to.isoformat(), ticker_from])\n",
    "        tickers_v4.loc[index_to, \"start_date\"] = start_date_from\n",
    "        tickers_v4.loc[index_to, \"start_data\"] = start_data_from\n",
    "\n",
    "        # Do the actual merging\n",
    "        from_ = pd.read_parquet(DATA_PATH + f\"processed/m1/{id_from}.parquet\")\n",
    "        to = pd.read_parquet(DATA_PATH + f\"processed/m1/{id_to}.parquet\")\n",
    "\n",
    "        # Because companies like to be annoying, a split/dividend can take place at the same time as a ticker change. We have to account for this. An example is TYDE -> OCTO with a 50:1 reverse split. However this is much easier than 5_process_raw_data.ipynb, because there is at most a single split. This is rare, but there are still a handful of cases.\n",
    "        if os.path.isfile(DATA_PATH + f\"raw/adjustments/{ticker_to}.csv\"):\n",
    "            adjustments = pd.read_csv(DATA_PATH + f\"raw/adjustments/{ticker_to}.csv\", parse_dates=True, index_col=0)\n",
    "            adjustments.index = pd.to_datetime(adjustments.index).date\n",
    "            adjustments = adjustments[(adjustments.index == start_data_to)]\n",
    "\n",
    "            # SPLIT ADJUSTMENT\n",
    "            split = adjustments[adjustments.type == 'SPLIT']\n",
    "            if len(split) > 0:\n",
    "                split_amount = split['amount'][0]\n",
    "\n",
    "                from_[['open', 'high', 'low', 'close']] = from_[['open', 'high', 'low', 'close']].multiply(split_amount)\n",
    "\n",
    "            # DIVIDEND ADJUSTMENT - REUN is the only case, not clear what happened there.\n",
    "            dividend = adjustments[adjustments.type == 'DIV']\n",
    "            if len(dividend) > 0:\n",
    "                print(ticker_to)\n",
    "                market_hours = get_market_calendar()\n",
    "                market_hours = market_hours[['regular_close']]\n",
    "\n",
    "                cum_div_date = end_data_from\n",
    "                cum_div_time = market_hours.loc[cum_div_date][0]\n",
    "                cum_div_datetime = datetime.combine(cum_div_date, cum_div_time)\n",
    "                cum_div_datetime = (from_[from_.index <= cum_div_datetime].index).max()\n",
    "                cum_div_close = from_.loc[cum_div_datetime, 'close']\n",
    "                dividend_amount = dividend['amount'][0]\n",
    "                    \n",
    "                adjustment_factor = 1 - dividend_amount/cum_div_close\n",
    "\n",
    "                from_[['open', 'high', 'low', 'close']] = from_[['open', 'high', 'low', 'close']].multiply(adjustment_factor)\n",
    "                from_ = round(from_, 4)\n",
    "                from_['turnover'] = from_['turnover'].astype(int)\n",
    "            \n",
    "            # ROUNDING\n",
    "            if len(split) > 0 or len(dividend) > 0:\n",
    "                from_ = round(from_, 4)\n",
    "                from_['turnover'] = from_['turnover'].astype(int)\n",
    "\n",
    "        # If on the old ticker, there are divs/splits on start_data_to (start of new ticker), then something is wrong.\n",
    "        if os.path.isfile(DATA_PATH + f\"raw/adjustments/{ticker_from}.csv\"):\n",
    "            adjustments = pd.read_csv(DATA_PATH + f\"raw/adjustments/{ticker_from}.csv\", parse_dates=True, index_col=0)\n",
    "            adjustments.index = pd.to_datetime(adjustments.index).date\n",
    "            adjustments = adjustments[(adjustments.index == start_data_to)]\n",
    "            assert len(adjustments) == 0\n",
    "\n",
    "        pd.concat([from_, to]).to_parquet(DATA_PATH + f\"processed/m1/{id_to}.parquet\", engine=\"fastparquet\", row_group_offsets=25000)\n",
    "        # pd.concat([from_, to]).to_parquet(DATA_PATH + f\"processed/m1/{new_id}.parquet\", engine=\"pyarrow\", compression = 'brotli', row_group_size = 25000)\n",
    "\n",
    "        # Delete old ticker from ticker list. You can also remove the files, but that is not necessary. In backtesting you will always loop over the ticker list and not over de files in the folder.\n",
    "        #os.remove(path = DATA_PATH + f\"processed/m1/{id_from}.parquet\") # Removal of old renamed ticker (not necessary)\n",
    "        tickers_v4.drop(index_from, inplace=True)\n",
    "        tickers_v4.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        # print(f\"Ticker change {ticker_from} -> {ticker_to} on {start_data_to} has been processed\")\n",
    "        # print(f\"{index_from/len(tickers_v4)*100:.1f}% | Length of tickers_v4 is {len(tickers_v4)}\")\n",
    "        break\n",
    "    \n",
    "    # If we have reached the end of the loop, it means we have processed everything. Then we can stop.\n",
    "    if index_from >= (len(tickers_v4)-1):\n",
    "        break\n",
    "\n",
    "# tickers_v4.to_csv(\"../data/tickers_v5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "738\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ticker</th>\n",
       "      <th>tickers_old</th>\n",
       "      <th>name</th>\n",
       "      <th>active</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>start_data</th>\n",
       "      <th>end_data</th>\n",
       "      <th>type</th>\n",
       "      <th>cik</th>\n",
       "      <th>composite_figi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4426</th>\n",
       "      <td>META-2022-06-09</td>\n",
       "      <td>META</td>\n",
       "      <td>[['2022-06-09', 'FB']]</td>\n",
       "      <td>Meta Platforms, Inc. Class A Common Stock</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-06-03</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>2019-06-03</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>CS</td>\n",
       "      <td>1326801.0</td>\n",
       "      <td>BBG000MM2P62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID ticker             tickers_old  \\\n",
       "4426  META-2022-06-09   META  [['2022-06-09', 'FB']]   \n",
       "\n",
       "                                           name  active  start_date  \\\n",
       "4426  Meta Platforms, Inc. Class A Common Stock    True  2019-06-03   \n",
       "\n",
       "        end_date  start_data    end_data type        cik composite_figi  \n",
       "4426  2024-03-01  2019-06-03  2024-03-01   CS  1326801.0   BBG000MM2P62  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers_v5 = get_tickers(v=5)\n",
    "renamings = tickers_v5[tickers_v5[\"tickers_old\"].str.len() > 2] # These were renamed\n",
    "print(len(renamings))\n",
    "tickers_v5[tickers_v5['ticker'] == 'META']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tickers that were renamed multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ticker</th>\n",
       "      <th>tickers_old</th>\n",
       "      <th>name</th>\n",
       "      <th>active</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>start_data</th>\n",
       "      <th>end_data</th>\n",
       "      <th>type</th>\n",
       "      <th>cik</th>\n",
       "      <th>composite_figi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2411</th>\n",
       "      <td>ERNA-2022-10-17</td>\n",
       "      <td>ERNA</td>\n",
       "      <td>[['2022-10-17', 'BTX'], ['2021-03-26', 'NTN']]</td>\n",
       "      <td>Eterna Therapeutics Inc. Common Stock</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-06-03</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>2019-06-03</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>CS</td>\n",
       "      <td>748592.0</td>\n",
       "      <td>BBG000CX5677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2751</th>\n",
       "      <td>FRGT-2022-05-27</td>\n",
       "      <td>FRGT</td>\n",
       "      <td>[['2022-05-27', 'HUSN'], ['2020-05-08', 'CIFS']]</td>\n",
       "      <td>Freight Technologies, Inc. Ordinary Shares</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-06-03</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>2019-06-03</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>CS</td>\n",
       "      <td>1687542.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID ticker  \\\n",
       "2411  ERNA-2022-10-17   ERNA   \n",
       "2751  FRGT-2022-05-27   FRGT   \n",
       "\n",
       "                                           tickers_old  \\\n",
       "2411    [['2022-10-17', 'BTX'], ['2021-03-26', 'NTN']]   \n",
       "2751  [['2022-05-27', 'HUSN'], ['2020-05-08', 'CIFS']]   \n",
       "\n",
       "                                            name  active  start_date  \\\n",
       "2411       Eterna Therapeutics Inc. Common Stock    True  2019-06-03   \n",
       "2751  Freight Technologies, Inc. Ordinary Shares    True  2019-06-03   \n",
       "\n",
       "        end_date  start_data    end_data type        cik composite_figi  \n",
       "2411  2024-03-01  2019-06-03  2024-03-01   CS   748592.0   BBG000CX5677  \n",
       "2751  2024-03-01  2019-06-03  2024-03-01   CS  1687542.0                 "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_renamings = tickers_v5[tickers_v5[\"tickers_old\"].str.len() > 30]\n",
    "print(len(multiple_renamings))\n",
    "multiple_renamings.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 5 tickers lists. These are:\n",
    "1. Basic ticker list with a lot of incorrect duplications.\n",
    "2. Duplications merged and incorrect tickers removed.\n",
    "3. ETFs added.\n",
    "4. Data start/end dates added.\n",
    "5. Renamings merged.\n",
    "Only the last should be used in backtesting.\n",
    "\n",
    "If Polygon just provided these from the start, it would have saved countless hours. But at least I learned some Python I guess. And at least Polygon does not ask thousands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.2 Updates\n",
    "Rerun the file after setting END_DATE and updating the list of renamings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algotrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
