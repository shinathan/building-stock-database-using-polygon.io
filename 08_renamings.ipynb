{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1 Merging ticker changes\n",
    "This is optional. If you want it ticker-centric or don't want to get a stockanalysis.com subscription, you can just skip this part.\n",
    "\n",
    "To get a list of ticker changes, We can loop through all tickers and query <code>Ticker Events</code> but this only works with non-delisted companies. And although you can infer it based on the ticker list by looking at whether the cik or figi has changed, that is very messy. Because a company can stay the same even if the ticker and cik/figi change. I actually did it, and it did found that it did not match the Polygon <code>Ticker Events</code>. Then I stumbled on [stockanalysis.com](https://stockanalysis.com/actions/changes/) where you can find all ticker changes for only 10 bucks a month. The first month is even free. You have to manually download them for each year and put them in the <code>stockanalysis/raw/ticker_changes/</code> folder.\n",
    "\n",
    "After merging those we will save the result to <code>raw/renamings.csv</code> which will contain the columns <code>['from', 'to', 'now', 'date']</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nathan\\miniconda3\\envs\\algotrading\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "from tickers import get_tickers, get_ticker_changes\n",
    "from times import get_market_dates, get_market_calendar, last_trading_date_before\n",
    "from datetime import datetime, date, time\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "DATA_PATH = \"../data/polygon/\"\n",
    "END_DATE = last_trading_date_before(date(2024, 4, 19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can be done once and then updates can be done with manually appending the list of ticker changes.\n",
    "# ###\n",
    "# # Aggregate the csv's\n",
    "# all_ticker_changes = []\n",
    "# for file in os.listdir(DATA_PATH + \"../stockanalysis/raw/ticker_changes/\"):\n",
    "#     ticker_changes_year = pd.read_csv(DATA_PATH + \"../stockanalysis/raw/ticker_changes/\" + file, \\\n",
    "#         parse_dates=True, index_col=0, usecols=[\"Date\", \"Old\", \"New\"])\n",
    "#     all_ticker_changes.append(ticker_changes_year)\n",
    "\n",
    "# ticker_changes = pd.concat(all_ticker_changes)\n",
    "# ticker_changes = pd.concat(all_ticker_changes)\n",
    "# ticker_changes.rename(columns={\"Old\": \"from\", \n",
    "#                                \"New\": \"to\"}, inplace=True)\n",
    "# ticker_changes.index.names = ['date']\n",
    "# ticker_changes.sort_index(inplace=True)\n",
    "# ticker_changes.to_csv(DATA_PATH + \"../stockanalysis/ticker_changes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4485\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4013</th>\n",
       "      <td>2022-06-09</td>\n",
       "      <td>FB</td>\n",
       "      <td>META</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date from    to\n",
       "4013  2022-06-09   FB  META"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_changes = pd.read_csv(DATA_PATH + \"../stockanalysis/ticker_changes.csv\")\n",
    "print(len(ticker_changes))\n",
    "ticker_changes[ticker_changes['from'] == \"FB\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somes there are special conditions, such as 'delinquent', which adds an extra letter at the end of the ticker. E.g. AAII went delinquent and then the ticker became AAIIE. However these are not real ticker changes so it is not contained in the stockanalysis database. However we can easily infer it from our own ticker list: if the dates are consecutive and an extra letter is added, we can infer the ticker change. We will save this to <code>raw/inferred_renamings.csv.</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a whopping 4347 ticker changes from 2003 to now that we have to take care of. But at least this was very easy to get.\n",
    "\n",
    "Now we will use them to merge our data. We have to be aware that it is possible for a ticker to used multiple times, so the <code>ticker_changes.csv</code> may contain multiple of the same tickers in the 'from' and 'to' column. \n",
    "\n",
    "After processing the ticker changes we will create a <code>tickers_v5.csv</code> which will be our definitive ticker list. This contains a column 'tickers_old', which will containa list of (date_of_change, ticker) pairs. So if A changes to B on day 2, and B changes to C on day 5, tickers_old for D will contain [[2, A], [5, B]].\n",
    "\n",
    "The process will be as follows:\n",
    "* As long as we have ticker changes to process\n",
    "    * Loop through <code>tickers_v4.csv</code>.\n",
    "        * Get the next trading date after 'end_date_data'.\n",
    "        * Search in <code>tickers_changes.csv</code> if there is a ticker change on this date.\n",
    "        * If it does:\n",
    "            * The stock data will be merged.\n",
    "            * In <code>tickers_v4.csv</code> we will change \"ticker\" to the new ticker and add a list [date, ticker] to \"tickers_old\".\n",
    "            * All other rows will be merged such as \"start_date\". For identifiers such as FIGI we will take the last available value. For the ID we will keep the original. If we do not do this, we might run into problems with identical IDs.\n",
    "            * The row of the old ticker will be deleted\n",
    "            * **We need to restart the loop!** If we don't the following can happen: Let's assume that a ticker was renamed from A -> B -> C -> D but that the order in which it appears in our ticker list is C, D, A, B. Using our loop, C gets merged with D. Then the loop checks D, which has no renamings. Then A gets merged with B. Then B gets merged with C, however that is incorrect! B should be merged with the new D, which contains C. Any double+ renamings have the risk of being in the 'wrong order'.\n",
    "                * For this to work, the ticker list must be sorted on end_date.\n",
    "            * Of course we must not forget that there can be adjustments on day 1 of the ticker change. There should be laws to prohibit this.\n",
    "\n",
    "Note: if a ticker A goes OTC and then comes back and changes to B, then we will have two files: one of the A before OTC and the A+B after OTC named B. This is as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_v4 = get_tickers(v=4)\n",
    "# QUICK BUG FIX, NEED TO REWRITE CODE TO MAKE IT CHRONIGICAL\n",
    "market_dates = get_market_dates()\n",
    "ticker_changes = get_ticker_changes()\n",
    "\n",
    "tickers_v4.insert(loc = 2, column = 'tickers_old', value = [{} for _ in range(len(tickers_v4))])\n",
    "\n",
    "while True:\n",
    "    tickers_v4 = tickers_v4.sort_values(by='end_data').reset_index(drop=True)\n",
    "\n",
    "    # tickers_v4 gets smaller by 1 element every time we run this loop.\n",
    "    for index_from, row_from in tickers_v4.copy().iterrows():\n",
    "        # Get values\n",
    "        type_from = row_from['type']\n",
    "        if type_from == \"INDEX\":\n",
    "            continue\n",
    "        id_from = row_from['ID']\n",
    "        ticker_from = row_from['ticker']\n",
    "        start_date_from = row_from['start_date']\n",
    "        end_date_from = row_from['end_date']\n",
    "        start_data_from = row_from['start_data']\n",
    "        end_data_from = row_from['end_data']\n",
    "\n",
    "        if end_data_from == END_DATE:\n",
    "            continue\n",
    "\n",
    "        start_data_to = market_dates[market_dates.index(end_data_from) + 1]\n",
    "\n",
    "        # Get ticker changes \n",
    "        change = ticker_changes[(ticker_changes.index == start_data_to) & (ticker_changes['from'] == ticker_from)]\n",
    "        if change.empty:\n",
    "            continue\n",
    "        elif len(change) > 1:\n",
    "            raise Exception(\"Duplicate!\")\n",
    "        ticker_to = change['to'].values[0]\n",
    "\n",
    "        # Set values of new ticker\n",
    "        row_to = tickers_v4[(tickers_v4['start_data'] == start_data_to) & (tickers_v4['ticker'] == ticker_to)]\n",
    "        if row_to.empty:\n",
    "            continue\n",
    "        index_to = row_to.index[0]\n",
    "        id_to = row_to['ID'].values[0]\n",
    "        tickers_v4.loc[index_to, \"tickers_old\"][start_data_to.isoformat()] = ticker_from\n",
    "        tickers_v4.loc[index_to, \"start_date\"] = start_date_from\n",
    "        tickers_v4.loc[index_to, \"start_data\"] = start_data_from\n",
    "\n",
    "        # Do the actual merging\n",
    "        from_ = pd.read_parquet(DATA_PATH + f\"processed/m1/{id_from}.parquet\")\n",
    "        to = pd.read_parquet(DATA_PATH + f\"processed/m1/{id_to}.parquet\")\n",
    "\n",
    "        # When a ticker changes, the adjustments carry over the the old ticker.\n",
    "            # Get market close minute to calculate the adjustment factor, and adjust 'to'.\n",
    "            # (Adjustment factor is the same throughout the day, so market close is arbitrarely chosen)\n",
    "        calendar = get_market_calendar('datetime')\n",
    "        start_data_to_dt = calendar.loc[start_data_to, 'regular_close']\n",
    "        start_data_to_dt_bar = to.loc[start_data_to_dt]\n",
    "        adjustment_factor = start_data_to_dt_bar['close'] / start_data_to_dt_bar['close_original']\n",
    "        from_[['open', 'high', 'low', 'close']] = from_[['open', 'high', 'low', 'close']].multiply(adjustment_factor)\n",
    "        from_ = round(from_, 4)\n",
    "\n",
    "        # Because companies like to be annoying, a split/dividend can take place at the same time as a ticker change. We have to account for this. An example is TYDE -> OCTO with a 50:1 reverse split. However this is much easier than 5_process_raw_data.ipynb, because there is at most a single split. This is rare, but there are still a handful of cases.\n",
    "        if os.path.isfile(DATA_PATH + f\"raw/adjustments/{ticker_to}.csv\"):\n",
    "            adjustments = pd.read_csv(DATA_PATH + f\"raw/adjustments/{ticker_to}.csv\", parse_dates=True, index_col=0)\n",
    "            adjustments.index = pd.to_datetime(adjustments.index).date\n",
    "            adjustments = adjustments[(adjustments.index == start_data_to)]\n",
    "\n",
    "            # SPLIT ADJUSTMENT\n",
    "            split = adjustments[adjustments.type == 'SPLIT']\n",
    "            if len(split) > 0:\n",
    "                split_amount = split['amount'][0]\n",
    "\n",
    "                from_[['open', 'high', 'low', 'close']] = from_[['open', 'high', 'low', 'close']].multiply(split_amount)\n",
    "\n",
    "            # DIVIDEND ADJUSTMENT - REUN is the only case, not clear what happened there, likely a 'special dividend'\n",
    "            dividend = adjustments[adjustments.type == 'DIV']\n",
    "            if len(dividend) > 0:\n",
    "                print(ticker_to)\n",
    "                market_hours = get_market_calendar()\n",
    "                market_hours = market_hours[['regular_close']]\n",
    "\n",
    "                cum_div_date = end_data_from\n",
    "                cum_div_time = market_hours.loc[cum_div_date][0]\n",
    "                cum_div_datetime = datetime.combine(cum_div_date, cum_div_time)\n",
    "                cum_div_datetime = (from_[from_.index <= cum_div_datetime].index).max()\n",
    "                cum_div_close = from_.loc[cum_div_datetime, 'close']\n",
    "                dividend_amount = dividend['amount'][0]\n",
    "                    \n",
    "                adjustment_factor = 1 - dividend_amount/cum_div_close\n",
    "\n",
    "                from_[['open', 'high', 'low', 'close']] = from_[['open', 'high', 'low', 'close']].multiply(adjustment_factor)\n",
    "            \n",
    "            # ROUNDING\n",
    "            if len(split) > 0 or len(dividend) > 0:\n",
    "                from_ = round(from_, 4)\n",
    "                from_['turnover'] = from_['turnover'].astype(int)\n",
    "\n",
    "        # # If on the old ticker, there are divs/splits on start_data_to (start of new ticker), then something is wrong.\n",
    "        # if os.path.isfile(DATA_PATH + f\"raw/adjustments/{ticker_from}.csv\"):\n",
    "        #     adjustments = pd.read_csv(DATA_PATH + f\"raw/adjustments/{ticker_from}.csv\", parse_dates=True, index_col=0)\n",
    "        #     adjustments.index = pd.to_datetime(adjustments.index).date\n",
    "        #     adjustments = adjustments[(adjustments.index == start_data_to)]\n",
    "        #     assert len(adjustments) == 0\n",
    "\n",
    "        # Remove the 'from' ticker, then paste the 'from' and 'to' ticker to m1_renamed for debugging purposes.\n",
    "        _ = shutil.move(DATA_PATH + f'processed/m1/{id_from}.parquet', DATA_PATH + f'processed/m1_renamed/{id_from}.parquet')\n",
    "        _ = shutil.copyfile(DATA_PATH + f'processed/m1/{id_to}.parquet', DATA_PATH + f'processed/m1_renamed/{id_to}.parquet')\n",
    "\n",
    "        pd.concat([from_, to]).to_parquet(DATA_PATH + f\"processed/m1/{id_to}.parquet\", engine=\"fastparquet\", row_group_offsets=25000)\n",
    "\n",
    "        tickers_v4.drop(index_from, inplace=True)\n",
    "        tickers_v4.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        print(f\"Ticker change {ticker_from} -> {ticker_to} on {start_data_to} has been processed\")\n",
    "        print(f\"{index_from/len(tickers_v4)*100:.1f}% | Length of tickers_v4 is {len(tickers_v4)}\")\n",
    "        break\n",
    "\n",
    "    # If we have reached the end of the loop, it means we have processed everything. Then we can stop.\n",
    "    if index_from >= (len(tickers_v4)-1):\n",
    "        break\n",
    "    \n",
    "tickers_v4 = tickers_v4.sort_values(by='ID').reset_index(drop=True)\n",
    "\n",
    "tickers_v4.to_csv(\"../data/tickers_v5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ticker</th>\n",
       "      <th>tickers_old</th>\n",
       "      <th>name</th>\n",
       "      <th>active</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>start_data</th>\n",
       "      <th>end_data</th>\n",
       "      <th>type</th>\n",
       "      <th>cik</th>\n",
       "      <th>composite_figi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9439</th>\n",
       "      <td>META-2022-06-09</td>\n",
       "      <td>META</td>\n",
       "      <td>{'2022-06-09': 'FB'}</td>\n",
       "      <td>Meta Platforms, Inc. Class A Common Stock</td>\n",
       "      <td>True</td>\n",
       "      <td>2012-05-18</td>\n",
       "      <td>2024-04-19</td>\n",
       "      <td>2012-05-18</td>\n",
       "      <td>2024-04-19</td>\n",
       "      <td>CS</td>\n",
       "      <td>1326801.0</td>\n",
       "      <td>BBG000MM2P62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID ticker           tickers_old  \\\n",
       "9439  META-2022-06-09   META  {'2022-06-09': 'FB'}   \n",
       "\n",
       "                                           name  active  start_date  \\\n",
       "9439  Meta Platforms, Inc. Class A Common Stock    True  2012-05-18   \n",
       "\n",
       "        end_date  start_data    end_data type        cik composite_figi  \n",
       "9439  2024-04-19  2012-05-18  2024-04-19   CS  1326801.0   BBG000MM2P62  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers_v5 = get_tickers(v=5)\n",
    "renamings = tickers_v5[tickers_v5[\"tickers_old\"].str.len() > 2] # These were renamed\n",
    "print(len(renamings))\n",
    "tickers_v5[tickers_v5['ticker'] == 'META']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tickers that were renamed multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ticker</th>\n",
       "      <th>tickers_old</th>\n",
       "      <th>name</th>\n",
       "      <th>active</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>start_data</th>\n",
       "      <th>end_data</th>\n",
       "      <th>type</th>\n",
       "      <th>cik</th>\n",
       "      <th>composite_figi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>ACNT-2022-08-11</td>\n",
       "      <td>ACNT</td>\n",
       "      <td>{'2022-08-11': 'SYNL', '2003-10-06': 'SYNC'}</td>\n",
       "      <td>Ascent Industries Co. Common Stock</td>\n",
       "      <td>True</td>\n",
       "      <td>2003-09-10</td>\n",
       "      <td>2024-04-19</td>\n",
       "      <td>2003-09-10</td>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>CS</td>\n",
       "      <td>95953.0</td>\n",
       "      <td>BBG000CLRHW7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>AGN-2015-06-15</td>\n",
       "      <td>AGN</td>\n",
       "      <td>{'2015-06-15': 'ACT', '2013-01-24': 'WPI'}</td>\n",
       "      <td>Allergan plc</td>\n",
       "      <td>False</td>\n",
       "      <td>2003-09-10</td>\n",
       "      <td>2020-05-08</td>\n",
       "      <td>2003-09-10</td>\n",
       "      <td>2020-05-08</td>\n",
       "      <td>CS</td>\n",
       "      <td>1578845.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ID ticker                                   tickers_old  \\\n",
       "200  ACNT-2022-08-11   ACNT  {'2022-08-11': 'SYNL', '2003-10-06': 'SYNC'}   \n",
       "442   AGN-2015-06-15    AGN    {'2015-06-15': 'ACT', '2013-01-24': 'WPI'}   \n",
       "\n",
       "                                   name  active  start_date    end_date  \\\n",
       "200  Ascent Industries Co. Common Stock    True  2003-09-10  2024-04-19   \n",
       "442                        Allergan plc   False  2003-09-10  2020-05-08   \n",
       "\n",
       "     start_data    end_data type        cik composite_figi  \n",
       "200  2003-09-10  2024-04-18   CS    95953.0   BBG000CLRHW7  \n",
       "442  2003-09-10  2020-05-08   CS  1578845.0                 "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_renamings = tickers_v5[tickers_v5[\"tickers_old\"].str.len() > 30]\n",
    "print(len(multiple_renamings))\n",
    "multiple_renamings.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 5 tickers lists. These are:\n",
    "1. Basic ticker list with a lot of incorrect duplications.\n",
    "2. Duplications merged and incorrect tickers removed.\n",
    "3. ETFs added.\n",
    "4. Data start/end dates added.\n",
    "5. Renamings merged.\n",
    "Only the last should be used in backtesting.\n",
    "\n",
    "If Polygon just provided these from the start, it would have saved countless hours. But at least I learned some Python I guess. And at least Polygon does not ask thousands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.2 Updates\n",
    "Rerun the file after setting END_DATE and updating the list of renamings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algotrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
