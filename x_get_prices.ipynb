{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading raw prices\n",
    "Because we already created functions in notebook 1 to download 1-minute tickers, this is easy. We need to keep in mind that a lot of stocks such as SPACs have almost no price data. So there are a lot of empty bars. Some stocks don't even have 1 trade for the entire day. However, I cannot remove them either. Because if for one week the stock has a lot of activity, my trading systems may still trade it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polygon.rest import RESTClient\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from utils import get_tickers, download_m1_raw_data, datetime_to_unix\n",
    "import pytz\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATA_PATH = \"../../../data/polygon/\"\n",
    "\n",
    "with open(DATA_PATH + \"secret.txt\") as f:\n",
    "    KEY = next(f).strip()\n",
    "\n",
    "client = RESTClient(api_key=KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-parallel approach**\n",
    "\n",
    "Downloading 100 stocks for 2021-01 to 2023-08 took 10 minutes and took 538 MB. To download all stocks (assume 6000) takes 600 minutes or 10 hours or 32 GB. To download from 2004 up to now takes around 70 hours or 224 GB.\n",
    "\n",
    "So instead of csv's we use <code>Parquet</code>. This saves a lot of disk space while sacrificing human readibility. Although you can just use [tad](https://www.tadviewer.com/) (does not work with brotli compression) or [ParquetViewer](https://github.com/mukunku/ParquetViewer/releases). And pandas already supports reading from Parquet files. Parquet only takes up 37.6% of the csv size based on a small sample. The parquet database (default snappy compression) from 2021-01 to 2023-08 takes up 13.8 GB in space, which would have taken 36.7 GB with csv's. To save even more disk space, I use the brotli compression algorithm, which only has 60% of the size of the standard snappy compression. However it is x1.28 slower. That seems like a good trade-off. Also, reading is very fast (~0.1 sec for almost 5 years of AAPL).\n",
    "\n",
    "In the future when I know SQL I will build a proper database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = get_tickers(v=3)\n",
    "tickers = tickers[tickers[\"type\"].isin([\"CS\", \"ADRC\", \"ETF\", \"ETN\", \"ETV\"])]\n",
    "tickers.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# For timing\n",
    "length = len(tickers)\n",
    "start_time = datetime.now()\n",
    "total_days_to_download = (tickers.end_date - tickers.start_date).sum()\n",
    "downloaded_days = timedelta(0)\n",
    "\n",
    "for index, row in tickers.iterrows():\n",
    "    id = row[\"ID\"]\n",
    "    ticker = row[\"ticker\"]\n",
    "    start_date = row[\"start_date\"]\n",
    "    end_date = row[\"end_date\"]\n",
    "\n",
    "    download_m1_raw_data(ticker = ticker, from_ = start_date, to = end_date, columns = [\"open\", \"high\", \"low\", \"close\", \"volume\"], path = DATA_PATH + f\"raw/m1/{id}.parquet\", client=client, to_parquet=True)\n",
    "\n",
    "    # For timing (becomes accurate after 5.0%)\n",
    "    passed_time = datetime.now() - start_time\n",
    "    days_just_downloaded = end_date - start_date\n",
    "    \n",
    "    total_days_to_download -= days_just_downloaded\n",
    "    downloaded_days += days_just_downloaded\n",
    "    used_time_per_day = passed_time/downloaded_days\n",
    "\n",
    "    remaining_time = used_time_per_day*total_days_to_download\n",
    "    remaining_hours = int(remaining_time.total_seconds()/3600)\n",
    "    remaining_minutes = int((remaining_time.total_seconds()%3600)/60)\n",
    "\n",
    "    print(f\"Progress: {round(index/length*100, 1)}% | ETA: {remaining_hours} hours and {remaining_minutes} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv(DATA_PATH + f\"raw/m1/A-2021-01-01.csv\", index_col=\"datetime\")\n",
    "# pd.read_parquet(DATA_PATH + f\"raw/m1/A-2021-01-01.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parallel approach**\n",
    "\n",
    "The Polygon API uses the <code>requests</code> library which does not support asynchrounous processing. So I have to use <code>aiohttp</code> and work with raw requests. Also we have to bother with pagination because of the 50000 limit.\n",
    "\n",
    "I used ChatGPT to convert the code above to work with aiohttp. However it did not work. After manually debugging, I got it working and the file is now in <code>3_get_prices_parallel.py</code>. You can specify the maximum amount of parallel requests. Setting it to 10 should make downloading 10 times faster. Be wary to not generate too many requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updates\n",
    "Not implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algotrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
