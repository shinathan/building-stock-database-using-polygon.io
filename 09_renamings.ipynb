{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.1 Merging ticker changes\n",
    "In section 1.7 we have acquired the list of renamings. Now we will use them to merge our data. We have to be aware that it is possible for a ticker to used multiple times, so the <code>ticker_changes.csv</code> may contain multiple of the same tickers in the 'from' and 'to' column. \n",
    "\n",
    "After processing the ticker changes we will create a <code>tickers_v5.csv</code> which will be our definitive ticker list. This contains a column 'tickers_old', which will containa list of (date_of_change, ticker) pairs. So if A changes to B on day 2, and B changes to C on day 5, tickers_old for D will contain [[2, A], [5, B]].\n",
    "\n",
    "The process will be as follows:\n",
    "* As long as we have ticker changes to process\n",
    "    * Loop through <code>tickers_v4.csv</code>.\n",
    "        * Get the next trading date after 'end_date_data'.\n",
    "        * Search in <code>tickers_changes.csv</code> if there is a ticker change on this date.\n",
    "        * If it does:\n",
    "            * The stock data will be merged.\n",
    "            * In <code>tickers_v4.csv</code> we will change \"ticker\" to the new ticker and add a list [date, ticker] to \"tickers_old\".\n",
    "            * All other rows will be merged such as \"start_date\". For identifiers such as FIGI we will take the last available value. For the ID we will use the new ticker with the combined start_date. So this means that in our <code>processed/m1/</code> folder there will be all tickers including merged ones. So the old FB, the old META and the merged FB+META named <code>META-yyyy-mm-dd.csv</code> will be in the same folder. You could delete the old FB and META but I keep them for debugging purposes.\n",
    "            * The row of the old ticker will be deleted\n",
    "            * **We need to restart the loop!** If we don't the following can happen: Let's assume that a ticker was renamed from A -> B -> C -> D but that the order in which it appears in our ticker list is C, D, A, B. Using our loop, C gets merged with D. Then the loop checks D, which has no renamings. Then A gets merged with B. Then B gets merged with C, however that is incorrect! B should be merged with the new D, which contains C. Any double+ renamings have the risk of being in the 'wrong order'.\n",
    "\n",
    "Note: if a ticker A goes OTC and then comes back and changes to B, then we will have two files: one of the A before OTC and the A+B after OTC named B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_tickers, get_market_dates, get_ticker_changes, get_market_hours\n",
    "from datetime import datetime, date, time\n",
    "import mplfinance as mpf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "DATA_PATH = \"../../../data/polygon/\"\n",
    "END_DATE = date(2023, 9, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_v4 = get_tickers(v=4)\n",
    "market_dates = get_market_dates()\n",
    "ticker_changes = get_ticker_changes()\n",
    "\n",
    "tickers_v4.insert(loc = 2, column = 'tickers_old', value = [[] for _ in range(len(tickers_v4))])\n",
    "\n",
    "while True:\n",
    "    # tickers_v4 gets smaller by 1 element every time we run this loop.\n",
    "    for index_from, row_from in tickers_v4.copy().iterrows():\n",
    "        # Get values\n",
    "        type_from = row_from['type']\n",
    "        if type_from == \"INDEX\":\n",
    "            continue\n",
    "        id_from = row_from['ID']\n",
    "        ticker_from = row_from['ticker']\n",
    "        start_date_from = row_from['start_date']\n",
    "        end_date_from = row_from['end_date']\n",
    "        start_data_from = row_from['start_data']\n",
    "        end_data_from = row_from['end_data']\n",
    "        if end_data_from == END_DATE:\n",
    "            continue\n",
    "\n",
    "        start_data_to = market_dates[market_dates.index(end_data_from) + 1]\n",
    "\n",
    "        # Get ticker changes \n",
    "        change = ticker_changes[(ticker_changes.index == start_data_to) & (ticker_changes['from'] == ticker_from)]\n",
    "        if change.empty:\n",
    "            continue\n",
    "        elif len(change) > 1:\n",
    "            raise Exception(\"Duplicate!\")\n",
    "        ticker_to = change['to'].values[0]\n",
    "\n",
    "        # Set values of new ticker\n",
    "        row_to = tickers_v4[(tickers_v4['start_data'] == start_data_to) & (tickers_v4['ticker'] == ticker_to)]\n",
    "        if row_to.empty:\n",
    "            continue\n",
    "        index_to = row_to.index[0]\n",
    "        id_to = row_to['ID'].values[0]\n",
    "        new_id = ticker_to + \"-\" + start_date_from.isoformat()\n",
    "        tickers_v4.loc[index_to, \"ID\"] = new_id\n",
    "        tickers_v4.loc[index_to, \"tickers_old\"].append([start_data_to.isoformat(), ticker_from])\n",
    "        tickers_v4.loc[index_to, \"start_date\"] = start_date_from\n",
    "        tickers_v4.loc[index_to, \"start_data\"] = start_data_from\n",
    "\n",
    "        # Do the actual merging\n",
    "        from_ = pd.read_parquet(DATA_PATH + f\"processed/m1/{id_from}.parquet\")\n",
    "        to = pd.read_parquet(DATA_PATH + f\"processed/m1/{id_to}.parquet\")\n",
    "\n",
    "        # Because companies like to be annoying, a split/dividend can take place at the same time as a ticker change. We have to account for this. An example is TYDE -> OCTO with a 50:1 reverse split. However this is much easier than 5_process_raw_data.ipynb, because there is at most a single split. This is rare, but there are still a handful of cases.\n",
    "        if os.path.isfile(DATA_PATH + f\"raw/adjustments/{ticker_to}.csv\"):\n",
    "            adjustments = pd.read_csv(DATA_PATH + f\"raw/adjustments/{ticker_to}.csv\", parse_dates=True, index_col=0)\n",
    "            adjustments.index = pd.to_datetime(adjustments.index).date\n",
    "            adjustments = adjustments[(adjustments.index == start_data_to)]\n",
    "\n",
    "            # SPLIT ADJUSTMENT\n",
    "            split = adjustments[adjustments.type == 'SPLIT']\n",
    "            if len(split) > 0:\n",
    "                split_amount = split['amount'][0]\n",
    "\n",
    "                from_[['open', 'high', 'low', 'close']] = from_[['open', 'high', 'low', 'close']].multiply(split_amount)\n",
    "                from_['volume'] = from_['volume'].divide(split_amount)\n",
    "\n",
    "            # DIVIDEND ADJUSTMENT - REUN is the only case, not clear what happened there.\n",
    "            dividend = adjustments[adjustments.type == 'DIV']\n",
    "            if len(dividend) > 0:\n",
    "                print(ticker_to)\n",
    "                market_hours = get_market_hours()\n",
    "                market_hours = market_hours[['regular_close']]\n",
    "\n",
    "                cum_div_date = end_data_from\n",
    "                cum_div_time = market_hours.loc[cum_div_date][0]\n",
    "                cum_div_datetime = datetime.combine(cum_div_date, cum_div_time)\n",
    "                cum_div_datetime = (from_[from_.index <= cum_div_datetime].index).max()\n",
    "                cum_div_close = from_.loc[cum_div_datetime, 'close']\n",
    "                dividend = dividend['amount'][0]\n",
    "                    \n",
    "                adjustment_factor = 1 - dividend/cum_div_close\n",
    "\n",
    "                from_[['open', 'high', 'low', 'close']] = from_[['open', 'high', 'low', 'close']].multiply(adjustment_factor)\n",
    "                from_['volume'] = from_['volume'].divide(adjustment_factor)\n",
    "                from_ = round(from_, 4)\n",
    "                from_['volume'] = from_['volume'].astype(int)\\\n",
    "            \n",
    "            # ROUNDING\n",
    "            if len(split) > 0 or len(dividend) > 0:\n",
    "                from_ = round(from_, 4)\n",
    "                from_['volume'] = from_['volume'].astype(int)\n",
    "\n",
    "        # If on the old ticker, there are divs/splits on start_data_to (start of new ticker), then something is wrong.\n",
    "        if os.path.isfile(DATA_PATH + f\"raw/adjustments/{ticker_from}.csv\"):\n",
    "            adjustments = pd.read_csv(DATA_PATH + f\"raw/adjustments/{ticker_from}.csv\", parse_dates=True, index_col=0)\n",
    "            adjustments.index = pd.to_datetime(adjustments.index).date\n",
    "            adjustments = adjustments[(adjustments.index == start_data_to)]\n",
    "            assert len(adjustments) == 0\n",
    "\n",
    "        pd.concat([from_, to]).to_parquet(DATA_PATH + f\"processed/m1/{new_id}.parquet\", engine=\"pyarrow\", compression = 'brotli')\n",
    "\n",
    "        # Delete old ticker from ticker list. You can also remove the files, but that is not necessary. In backtesting you will always loop over the ticker list and not over de files in the folder.\n",
    "        # os.remove(path = DATA_PATH + f\"processed/m1/{id_from}.parquet\") # Removal of old renamed ticker (not necessary)\n",
    "        # os.remove(path = DATA_PATH + f\"processed/m1/{id_to}.parquet\") # Remove of new ticker (not necessary)\n",
    "        tickers_v4.drop(index_from, inplace=True)\n",
    "        tickers_v4.reset_index(inplace=True, drop=True)\n",
    "        print(f\"Ticker change {ticker_from} -> {ticker_to} on {start_data_to} has been processed\")\n",
    "        print(f\"{index_from/len(tickers_v4)*100:.1f}% | Length of tickers_v4 is {len(tickers_v4)}\")\n",
    "        break\n",
    "    \n",
    "    # If we have reached the end of the loop, it means we have processed everything. Then we can stop.\n",
    "    if index_from >= (len(tickers_v4)-1):\n",
    "        break\n",
    "\n",
    "tickers_v4.to_csv(\"../../../data/tickers_v5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ticker</th>\n",
       "      <th>tickers_old</th>\n",
       "      <th>name</th>\n",
       "      <th>active</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>start_data</th>\n",
       "      <th>end_data</th>\n",
       "      <th>type</th>\n",
       "      <th>cik</th>\n",
       "      <th>composite_figi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3976</th>\n",
       "      <td>META-2019-01-01</td>\n",
       "      <td>META</td>\n",
       "      <td>[['2022-06-09', 'FB']]</td>\n",
       "      <td>Meta Platforms, Inc. Class A Common Stock</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2023-09-01</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>2023-09-01</td>\n",
       "      <td>CS</td>\n",
       "      <td>1326801.0</td>\n",
       "      <td>BBG000MM2P62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID ticker             tickers_old  \\\n",
       "3976  META-2019-01-01   META  [['2022-06-09', 'FB']]   \n",
       "\n",
       "                                           name  active  start_date  \\\n",
       "3976  Meta Platforms, Inc. Class A Common Stock    True  2019-01-01   \n",
       "\n",
       "        end_date  start_data    end_data type        cik composite_figi  \n",
       "3976  2023-09-01  2019-01-02  2023-09-01   CS  1326801.0   BBG000MM2P62  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers_v5 = get_tickers(v=5)\n",
    "renamings = tickers_v5[tickers_v5[\"tickers_old\"].str.len() > 2] # These were renamed\n",
    "print(len(renamings))\n",
    "tickers_v5[tickers_v5['ticker'] == 'META']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tickers that were renamed multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ticker</th>\n",
       "      <th>tickers_old</th>\n",
       "      <th>name</th>\n",
       "      <th>active</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>start_data</th>\n",
       "      <th>end_data</th>\n",
       "      <th>type</th>\n",
       "      <th>cik</th>\n",
       "      <th>composite_figi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2162</th>\n",
       "      <td>ERNA-2019-01-01</td>\n",
       "      <td>ERNA</td>\n",
       "      <td>[['2022-10-17', 'BTX'], ['2021-03-26', 'NTN']]</td>\n",
       "      <td>Eterna Therapeutics Inc. Common Stock</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2023-09-01</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>2023-09-01</td>\n",
       "      <td>CS</td>\n",
       "      <td>748592.0</td>\n",
       "      <td>BBG000CX5677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3448</th>\n",
       "      <td>JXJT-2019-01-01</td>\n",
       "      <td>JXJT</td>\n",
       "      <td>[['2022-10-10', 'LLL'], ['2021-10-13', 'KBSF']]</td>\n",
       "      <td>JX Luxventure Limited Common Stock</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>2023-09-01</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>2023-09-01</td>\n",
       "      <td>CS</td>\n",
       "      <td>1546383.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID ticker                                      tickers_old  \\\n",
       "2162  ERNA-2019-01-01   ERNA   [['2022-10-17', 'BTX'], ['2021-03-26', 'NTN']]   \n",
       "3448  JXJT-2019-01-01   JXJT  [['2022-10-10', 'LLL'], ['2021-10-13', 'KBSF']]   \n",
       "\n",
       "                                       name  active  start_date    end_date  \\\n",
       "2162  Eterna Therapeutics Inc. Common Stock    True  2019-01-01  2023-09-01   \n",
       "3448     JX Luxventure Limited Common Stock    True  2019-01-01  2023-09-01   \n",
       "\n",
       "      start_data    end_data type        cik composite_figi  \n",
       "2162  2019-01-02  2023-09-01   CS   748592.0   BBG000CX5677  \n",
       "3448  2019-01-02  2023-09-01   CS  1546383.0                 "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_renamings = tickers_v5[tickers_v5[\"tickers_old\"].str.len() > 30]\n",
    "print(len(multiple_renamings))\n",
    "multiple_renamings.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 5 tickers lists. These are:\n",
    "1. Basic ticker list with a lot of incorrect duplications.\n",
    "2. Duplications merged and incorrect tickers removed.\n",
    "3. ETFs added.\n",
    "4. Data start/end dates added.\n",
    "5. Renamings merged.\n",
    "Only the last should be used in backtesting.\n",
    "\n",
    "If Polygon just provided these from the start, it would have saved countless hours. But at least I learned some Python I guess. And at least Polygon does not ask thousands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.2 Updates\n",
    "Rerun the file after setting END_DATE and updating the list of renamings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algotrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
