{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import remove_extended_hours, get_market_dates, get_tickers, get_data, first_trading_date_after_equal\n",
    "from datetime import datetime, date, timedelta\n",
    "import mplfinance as mpf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import ast\n",
    "DATA_PATH = \"../../../data/polygon/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Parquet Files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Simulating screeners\n",
    "In backtesting, we should be able to simulate what a screener would have given, else we would have to simulate all stocks simultaneously. Because this is a computationally expensive thing to do, the results are saved in the <code>processed/cache/</code> folder. For the short-term (<1d), simulated scans should match real scans. I will use the IBKR TWS scanner to determine what is possible. The screen can be combined (just like with real screens). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function for calculating the most liquid N non-ADR stocks. This is to simulate the Russell or S&P indices. Of course there will be differences, but I care more about liquidity than whether it is precisely in an index or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_top_n_liquid(n=3000, start = date(2000, 1, 1), end = date(2100, 1, 1)):\n",
    "    tickers = get_tickers()\n",
    "    tickers = tickers[tickers['type'] == \"CS\"]\n",
    "    data = pd.DataFrame(index=get_market_dates())\n",
    "    IDs = []\n",
    "\n",
    "    quarterly_all = pd.DataFrame()\n",
    "    for index, id in enumerate(tickers['ID']):\n",
    "        bars = get_data(id, columns=['volume', 'close'], start=start, end=end)\n",
    "        quarterly = bars.resample('Q').agg({'close': 'last',\n",
    "                                'volume': 'sum'})\n",
    "        quarterly['turnover'] = quarterly['volume'] * quarterly['close']\n",
    "        quarterly = quarterly.rename(columns={'turnover': id}).drop(columns=['volume', 'close'])\n",
    "        quarterly_all = quarterly_all.merge(quarterly[id], how='outer', left_index=True, right_index=True)\n",
    "        #print(index)\n",
    "        \n",
    "        # Avoids defragmentation, increasing performance. Without this it would take more than 4x longer.\n",
    "        if index % 100 == 0:\n",
    "            quarterly_all = quarterly_all.copy()\n",
    "\n",
    "    for _, row in quarterly_all.copy().iterrows():\n",
    "        IDs.append(row[row.notna()].nlargest(n).index.tolist()) # Calculate largest N stocks, append to 'IDs'\n",
    "        \n",
    "    quarterly_all['IDs'] = IDs\n",
    "    quarterly_all = quarterly_all[['IDs']] # Get only the IDs\n",
    "    quarterly.index = quarterly.index + timedelta(days=1) # Get first day of new month to avoid look-ahead bias\n",
    "    quarterly.index = map(first_trading_date_after_equal, quarterly.index.date) # Get first trading day of new month\n",
    "\n",
    "    data = data.merge(quarterly_all, how='left', left_index=True, right_index=True) # Convert to daily\n",
    "    data = data.fillna(method='ffill')\n",
    "    data.index = pd.to_datetime(data.index)\n",
    "\n",
    "    data.to_csv(DATA_PATH + f'processed/cache/top_{n}_liquid.csv')\n",
    "    return\n",
    "\n",
    "def get_top_n_liquid(day, n=3000):\n",
    "    if os.path.isfile(DATA_PATH + f'processed/cache/top_{n}_liquid.csv'):\n",
    "        data = pd.read_csv(DATA_PATH + f'processed/cache/top_{n}_liquid.csv', index_col=0)\n",
    "        data.index = pd.to_datetime(data.index).date\n",
    "        if data.index[-1] < day:\n",
    "            data = calculate_top_n_liquid(n)\n",
    "    else:\n",
    "        calculate_top_n_liquid(n)\n",
    "        data = pd.read_csv(DATA_PATH + f'processed/cache/top_{n}_liquid.csv', index_col=0)\n",
    "        data.index = pd.to_datetime(data.index).date\n",
    "    \n",
    "    return ast.literal_eval(data.loc[day, 'IDs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TSLA-2019-01-01', 'NVDA-2019-01-01', 'AAPL-2019-01-01', 'MSFT-2019-01-01', 'AMD-2019-01-01']\n"
     ]
    }
   ],
   "source": [
    "data = get_top_n_liquid(date(2023, 8, 25), n=500)\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top losers %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-market liquid (premarket open to 9:25)\n",
    "# Uses m5 data\n",
    "# DataFrame with lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intraday P30 in 15-minutes with constraints\n",
    "# Uses m1 data\n",
    "# DataFrame with lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Testing a mean-reversion strategy on SPY.\n",
    "I will try out a popular strategy using the IBS indicator. I want to see if I can match the results on [here](https://www.quantifiedstrategies.com/internal-bar-strength-ibs-indicator-strategy/). I will also use pure pandas.\n",
    "\n",
    "* Universe: SPY ETF\n",
    "* Entry: IBS < 0.2\n",
    "* Exit: IBS > 0.8\n",
    "* Trading on close prices.\n",
    "\n",
    "Although it is possible to get the exact closing price using market-on-close orders, you cannot know the value of the IBS at market close. So as an extension I will look at how the strategy has performed if the IBS is calculated using daily bars exluding the last minute.\n",
    "\n",
    "I also want to look at the impact of taxes (36%) and inflation.\n",
    "\n",
    "Note: I would not use the SPY for trading indices, because futures are more liquid. Even index CFDs may have lower trading costs. Also, I can only access US-domiciled ETFs from a very small amount of brokers due to stupid EU regulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My focus will be on:\n",
    "* Single or low-asset strategies for e.g. SPY/VXX. The actual execution may not be in the ETF but in a more liquid futures.\n",
    "* Screenable strategies, e.g. S3 or Gap-up short. These use the top % winners and losers. At most the backtester has to keep track of 5-10 stocks at the same time. These may use fundamentals which I will implement later.\n",
    "* All systems have a expected holding time of at least 1 hour.\n",
    "* As such, I do not need tick data for signal generation. However I want it to simulate realistic fills and to clean data.\n",
    "\n",
    "I care less about:\n",
    "* Mostly fundamental strategies\n",
    "* Scalping: only feasible for US-based PFOF brokers. Scalping could use lower resolution bars (15s) but always use ticks to simulate fills. My backtester should always be able to handle fixed-time-interval bars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algotrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
