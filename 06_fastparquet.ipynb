{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Parquet files\n",
    "Since we will be working with close to 10000 parquet files we need to make sure that we can read it fast. \n",
    "\n",
    "With the default settings, it is not possible to partially read a Parquet file. This is bad, because most of the time we only need a small subsection of the data.\n",
    "\n",
    "Parquet works with row groups. By default the entire file is one group (or 50 million by default) and hence a partial read is not possible. If there are multiple row groups, it may be faster to do a partial read. We can specify the <code>row_group_size</code> in the <code>to_parquet</code> method to be a smaller amount than the default 50 million. I will test whether it is actually faster to do a partial read. I will set the row group size to 25000, which for most stocks on the m1 timeframe is 1 quarter of data.\n",
    "\n",
    "With fastparquet we can also append, which is not possible with pyarrow. So I will use fastparquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_tickers\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "DATA_PATH = \"../data/polygon/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = get_tickers(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read speeds\n",
    "We will compare read speeds for fastparquet (gzip) for large and small row groups. We will also compare partial reads versus full reads. At the end we test the read speeds of the csv format.\n",
    "\n",
    "Large groups: read all (0.7s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tickers.iterrows():\n",
    "    id = row['ID']\n",
    "    dataset = pq.ParquetDataset(\n",
    "    DATA_PATH + f\"raw/m1 (fastparquet snappy no limit)/{id}.parquet\")\n",
    "    df = dataset.read().to_pandas()\n",
    "    if index > 24:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large groups: read part (0.5s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tickers.iterrows():\n",
    "    id = row['ID']\n",
    "    dataset = pq.ParquetDataset(\n",
    "    DATA_PATH + f\"raw/m1 (fastparquet snappy no limit)/{id}.parquet\",\n",
    "    filters=[(\"datetime\", \">=\", date(2023, 8, 1)), (\"datetime\", \"<=\", date(2023, 9, 1))])\n",
    "    df = dataset.read().to_pandas()\n",
    "    if index > 24:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small groups: read all (0.7s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tickers.iterrows():\n",
    "    id = row['ID']\n",
    "    dataset = pq.ParquetDataset(\n",
    "    DATA_PATH + f\"raw/m1 (fastparquet snappy 25000)/{id}.parquet\")\n",
    "    df = dataset.read().to_pandas()\n",
    "    if index > 24:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small groups: read part (0.1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tickers.iterrows():\n",
    "    id = row['ID']\n",
    "    dataset = pq.ParquetDataset(\n",
    "    DATA_PATH + f\"raw/m1 (fastparquet snappy 25000)/{id}.parquet\",\n",
    "    filters=[(\"datetime\", \">=\", date(2023, 8, 1)), (\"datetime\", \"<=\", date(2023, 9, 1))])\n",
    "    df = dataset.read().to_pandas()\n",
    "    if index > 24:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Csv: read all (7.1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tickers.iterrows():\n",
    "    id = row['ID']\n",
    "    dataset = pd.read_csv(DATA_PATH + f\"raw/m1 (csv)/{id}.csv\")\n",
    "    if index > 24:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed partitioning it in small row groups increases the speed drastically for partial reads. Under the hood it works like this: every row group has its own statistics such as the minimum/maximum. When we want to select a specific subset, Parquet compares the value to the statistics of all row groups. If there is only one row group, it has to 'open' the entire file. If there are multiple row groups, Parquet can quickly select which row group is the correct one. Then only the small part (25000) of rows is read.\n",
    "\n",
    "Of course, specifying <code>row_group_size</code> to be small increases the amount of statistics that need to be stored and decreases the efficiency of compression. However partial reading is now 5x faster, while the file size is roughly the same.\n",
    "\n",
    "I also tested the writing speed for small and large group sizes. Small row group sizes take more time to compress. And if you go too small partial reads actually slow down. I eventually settled on a row group size of 25000, which is approximately 1 quarter of 1-minute data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write speeds\n",
    "We will compare snappy (default), gzip, brotli and csv write speeds and file size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snappy (default row groups of 50 million): 2.5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tickers.iterrows():\n",
    "    id = row['ID']\n",
    "    dataset = pq.ParquetDataset(\n",
    "    DATA_PATH + f\"raw/m1 (fastparquet gzip)/{id}.parquet\",\n",
    "    filters=[(\"datetime\", \">=\", date(2000, 1, 1)), (\"datetime\", \"<=\", date(2100, 1, 1))])\n",
    "    df = dataset.read().to_pandas()\n",
    "    \n",
    "    df.to_parquet(DATA_PATH + f\"raw/m1 (fastparquet snappy no limit)/{id}.parquet\", engine=\"fastparquet\", compression='snappy') #  row_group_size=25000\n",
    "\n",
    "    if index > 24:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snappy: 3.1s (124 MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tickers.iterrows():\n",
    "    id = row['ID']\n",
    "    dataset = pq.ParquetDataset(\n",
    "    DATA_PATH + f\"raw/m1 (fastparquet gzip)/{id}.parquet\",\n",
    "    filters=[(\"datetime\", \">=\", date(2000, 1, 1)), (\"datetime\", \"<=\", date(2100, 1, 1))])\n",
    "    df = dataset.read().to_pandas()\n",
    "    \n",
    "    df.to_parquet(DATA_PATH + f\"raw/m1 (fastparquet snappy 25000)/{id}.parquet\", engine=\"fastparquet\", compression='snappy', row_group_offsets=25000) #  row_group_size=25000\n",
    "\n",
    "    if index > 24:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gzip: 29s (80 MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tickers.iterrows():\n",
    "    id = row['ID']\n",
    "    dataset = pq.ParquetDataset(\n",
    "    DATA_PATH + f\"raw/m1 (fastparquet gzip)/{id}.parquet\",\n",
    "    filters=[(\"datetime\", \">=\", date(2000, 1, 1)), (\"datetime\", \"<=\", date(2100, 1, 1))])\n",
    "    df = dataset.read().to_pandas()\n",
    "    \n",
    "    df.to_parquet(DATA_PATH + f\"raw/m1 (fastparquet gzip)/{id}.parquet\", engine=\"fastparquet\", compression='gzip', row_group_offsets=25000) #  row_group_size=25000\n",
    "\n",
    "    if index > 24:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brotli: almost 10 minutes (63 MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tickers.iterrows():\n",
    "    id = row['ID']\n",
    "    dataset = pq.ParquetDataset(\n",
    "    DATA_PATH + f\"raw/m1 (fastparquet gzip)/{id}.parquet\",\n",
    "    filters=[(\"datetime\", \">=\", date(2000, 1, 1)), (\"datetime\", \"<=\", date(2100, 1, 1))])\n",
    "    df = dataset.read().to_pandas()\n",
    "    \n",
    "    df.to_parquet(DATA_PATH + f\"raw/m1 (fastparquet brotli)/{id}.parquet\", engine=\"fastparquet\", compression='brotli', row_group_offsets=25000) #  row_group_size=25000\n",
    "\n",
    "    if index > 24:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Csv: 38s (278 MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tickers.iterrows():\n",
    "    id = row['ID']\n",
    "    dataset = pq.ParquetDataset(\n",
    "    DATA_PATH + f\"raw/m1 (fastparquet gzip)/{id}.parquet\",\n",
    "    filters=[(\"datetime\", \">=\", date(2000, 1, 1)), (\"datetime\", \"<=\", date(2100, 1, 1))])\n",
    "    df = dataset.read().to_pandas()\n",
    "    \n",
    "    df.to_csv(DATA_PATH + f\"raw/m1 (csv)/{id}.csv\")\n",
    "\n",
    "    if index > 24:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I will use the default snappy compression with row size 25000. Even though it takes more disk space, it is much faster to write. Also we already save 55% in disk space compared to the standard csv format. Also I just bought a 4 TB SSD so I am not going to waste that xp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appending data to Parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare appending data versus reading + concatenating + writing. We will add A-2019-01-01 to AAPL-2019-01-01 together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "update = pq.ParquetDataset(\n",
    "DATA_PATH + f\"raw/m1 (fastparquet snappy 25000)/A-2019-01-01.parquet\")\n",
    "update = update.read().to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appending: 0.1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastparquet import write\n",
    "write(DATA_PATH + f\"raw/m1 (fastparquet snappy 25000)/AAPL-copy.parquet\", update, append=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading + concatenating + writing: 3.6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read AAPL\n",
    "original = pq.ParquetDataset(\n",
    "DATA_PATH + f\"raw/m1 (fastparquet snappy 25000)/AAPL-2019-01-01.parquet\")\n",
    "original = original.read().to_pandas()\n",
    "\n",
    "pd.concat([original, update]).to_parquet(DATA_PATH + f\"raw/m1 (fastparquet snappy 25000)/A-AAPL.parquet\", engine=\"fastparquet\", compression='snappy', row_group_offsets=2500) #  row_group_size=25000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appending is more than 30x faster than reading everything, concatenating and writing. And appending is something we will do for all tickers when we update the data, so this speedup is very meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algotrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
