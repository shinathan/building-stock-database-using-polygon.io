{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Downloading raw prices\n",
    "Because we already created functions in notebook 1 to download 1-minute tickers, this is easy. We need to keep in mind that a lot of stocks such as SPACs have almost no price data. So there are a lot of empty bars. Some stocks don't even have 1 trade for the entire day. However, I cannot remove them either. Because if for one week the stock has a lot of activity, my trading systems may still trade it. That is the curse of the HTB small caps space. I cannot just filter on monthly liquidity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "from polygon.rest import RESTClient\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from tickers import get_tickers\n",
    "from data import download_m1_raw_data\n",
    "import pandas as pd\n",
    "from fastparquet import write\n",
    "\n",
    "DATA_PATH = \"../data/polygon/\"\n",
    "\n",
    "with open(DATA_PATH + \"secret.txt\") as f:\n",
    "    KEY = next(f).strip()\n",
    "\n",
    "client = RESTClient(api_key=KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-parallel approach**\n",
    "\n",
    "Downloading outside of market hours is much faster.\n",
    "\n",
    "So instead of csv's we use <code>Parquet</code>. This saves a lot of disk space while sacrificing human readibility. Although you can just use [tad](https://www.tadviewer.com/) (fastparquet) or [ParquetViewer](https://github.com/mukunku/ParquetViewer/releases) (pyarrow). And pandas already supports reading from Parquet files. \n",
    "\n",
    "In <code>06_fastparquet.ipynb</code> you can see a comparison between parquet compression algorithms and csv's. We save more than 50% in disk space, while write speeds are more than x7.\n",
    "\n",
    "Downloading the data takes around 20 hours for data from 2019-01-01 to 2023-09-01. However this only has to be done once, after which you only update (append)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = get_tickers(v=3)\n",
    "tickers = tickers[tickers[\"type\"].isin([\"CS\", \"ADRC\", \"ETF\"])]\n",
    "tickers.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# For timing\n",
    "length = len(tickers)\n",
    "start_time = datetime.now()\n",
    "total_days_to_download = (tickers.end_date - tickers.start_date).sum()\n",
    "downloaded_days = timedelta(0)\n",
    "\n",
    "for index, row in tickers.iterrows():\n",
    "    id = row[\"ID\"]\n",
    "    ticker = row[\"ticker\"]\n",
    "\n",
    "    start_date = row[\"start_date\"]\n",
    "    end_date = row[\"end_date\"]\n",
    "\n",
    "    m1 = download_m1_raw_data(ticker = ticker, from_ = start_date, to = end_date, columns = [\"open\", \"high\", \"low\", \"close\", \"volume\"], client=client)\n",
    "    if m1 is None:\n",
    "        continue\n",
    "\n",
    "    m1.to_parquet(DATA_PATH + f\"raw/m1/{id}.parquet\", engine=\"fastparquet\", compression=\"snappy\", row_group_offsets=25000)\n",
    "\n",
    "    # For timing (becomes accurate after 5.0%)\n",
    "    passed_time = datetime.now() - start_time\n",
    "    days_just_downloaded = end_date - start_date\n",
    "    \n",
    "    total_days_to_download -= days_just_downloaded\n",
    "    downloaded_days += days_just_downloaded\n",
    "    used_time_per_day = passed_time/downloaded_days\n",
    "\n",
    "    remaining_time = used_time_per_day*total_days_to_download\n",
    "    remaining_hours = int(remaining_time.total_seconds()/3600)\n",
    "    remaining_minutes = int((remaining_time.total_seconds()%3600)/60)\n",
    "\n",
    "    print(f\"Progress: {round(index/length*100, 1)}% | ETA: {remaining_hours} hours and {remaining_minutes} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv(DATA_PATH + f\"raw/m1/A-2021-01-01.csv\", index_col=\"datetime\")\n",
    "# pd.read_parquet(DATA_PATH + f\"raw/m1/A-2021-01-01.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parallel approach (may not work now)**\n",
    "\n",
    "The Polygon API uses the <code>requests</code> library which does not support asynchronous processing. So I have to use <code>aiohttp</code> and work with raw requests. Also we have to bother with pagination because of the 50000 limit.\n",
    "\n",
    "I used ChatGPT to convert the code above to work with aiohttp. However it did not work. After manually debugging, I got it working and the file is now in <code>06_parallel.py</code>. You can specify the maximum amount of parallel requests. Setting it to 10 should make downloading 10 times faster. Be wary to not generate too many requests.\n",
    "\n",
    "By the way, it is possible to run multiple python notebook files in VSCode. Simply copy this file multiple times, filter on <code>index</code> and run them all. This speeds it up, because the bottleneck is not processing power but the speed of a request.\n",
    "\n",
    "Since April 2024 it is possible to download flat files from Polygon which will be MUCH faster than sending a gigantic amount of requests. However I am too lazy to update my code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Updates\n",
    "Run the first cell to import the modules and then run the cells below. Making a backup of the <code>raw/m1</code> folder is recommended! If something goes wrong in this step, you cannot go back without downloading everything again. Reminder: the only difference between tickers_v3 and tickers_v4 is that tickers_v4 removes the ghost tickers and has start/end dates for the data.\n",
    "\n",
    "old_END_DATE is the date up to which we have data.\n",
    "\n",
    "Loop through tickers_v3:\n",
    "- If the ID is in the (old) <code>tickers_v4</code> and <code>end_date</code>(v3) is larger than the old_END_DATE (v4), then we know this is a ticker that kept its listing. We can simply append the new data. This is true for most stocks.\n",
    "- If the ID is not in <code>tickers_v4</code> AND start_date (v3) is larger than old_END_DATE, we need to create a new file. This is a new listing. If it is smaller than old_END_DATE this means the ticker had no data in the first place because in tickers_v4 the ghost tickers are already removed.\n",
    "\n",
    "All other tickers are those that have been delisted and need no updating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_v4 = get_tickers(v=4, types=[\"CS\", \"ADRC\", \"ETF\"])\n",
    "tickers_v4.to_csv(\"../data/tickers_v4_OLD.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tickers_v4 = get_tickers(v=4, types=[\"CS\", \"ADRC\", \"ETF\"])\n",
    "old_END_DATE = old_tickers_v4['end_date'].max()\n",
    "old_IDs = list(old_tickers_v4['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_v3 = get_tickers(v=3, types=[\"CS\", \"ADRC\", \"ETF\"])\n",
    "\n",
    "for index, row in tickers_v3.iterrows():\n",
    "    id = row[\"ID\"]\n",
    "    ticker = row[\"ticker\"]\n",
    "    start_date = row[\"start_date\"]\n",
    "    end_date = row[\"end_date\"]\n",
    "\n",
    "    if id in old_IDs and end_date > old_END_DATE:\n",
    "        update = download_m1_raw_data(ticker = ticker, from_ = old_END_DATE + timedelta(days=1), to = end_date, columns = [\"open\", \"high\", \"low\", \"close\", \"volume\"], client=client)\n",
    "        if update is not None:\n",
    "            write(DATA_PATH + f\"raw/m1/{id}.parquet\", update, append=True, compression=\"snappy\", row_group_offsets=25000)\n",
    "            print(f'Updating {id}')\n",
    "\n",
    "    elif id not in old_IDs and start_date > old_END_DATE:\n",
    "        data = download_m1_raw_data(ticker = ticker, from_ = row[\"start_date\"], to = end_date, columns = [\"open\", \"high\", \"low\", \"close\", \"volume\"], client=client)\n",
    "        data.to_parquet(DATA_PATH + f\"raw/m1/{id}.parquet\", engine=\"fastparquet\", compression=\"snappy\", row_group_offsets=25000)\n",
    "        print(f'Downloading {id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flat files\n",
    "Goal: download the flatfiles and split them into individual ticker files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytz\n",
    "import gzip\n",
    "from pytz import timezone\n",
    "from times import get_market_dates\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "START_DATE = date(2003, 9, 10)\n",
    "END_DATE = date(2024, 4, 19)\n",
    "\n",
    "session = boto3.Session(\n",
    "   aws_access_key_id='7203c471-037b-4944-96b0-effc0d3911b3',\n",
    "   aws_secret_access_key='IOOFCMHAT7plpPitNmqFICLdG1AnhC5l',\n",
    ")\n",
    "s3 = session.client(\n",
    "   's3',\n",
    "   endpoint_url='https://files.polygon.io',\n",
    "   config=Config(signature_version='s3v4'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial download\n",
    "Download everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODELETE\n",
    "for day in get_market_dates(date(2004, 1, 1), END_DATE):\n",
    "    destination = DATA_PATH + f'raw/flatfiles/{day.isoformat()}.csv.gz'\n",
    "    s3.download_file('flatfiles', \n",
    "                f'us_stocks_sip/minute_aggs_v1/{day.year}/{day.strftime(\"%m\")}/{day.isoformat()}.csv.gz', \n",
    "                destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process year-by-year (this takes enormously long btw, however you only need to run it one time so I won't go into the hassle to optimize speed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2003, 2024+1):\n",
    "    files = []\n",
    "    for day in get_market_dates(date(year, 1, 1), date(year, 12, 31)):\n",
    "        destination = DATA_PATH + f'raw/flatfiles/{day.isoformat()}.csv.gz'\n",
    "        with gzip.open(destination) as f:\n",
    "            all_bars = pd.read_csv(f)\n",
    "            all_bars = all_bars[['window_start', 'ticker', 'open', 'high', 'low', 'close', 'volume']]\n",
    "            all_bars = all_bars.rename(columns={'window_start': 'datetime'})\n",
    "            all_bars = all_bars.set_index('datetime')\n",
    "            all_bars.index = pd.to_datetime(all_bars.index, unit='ns') # Convert to datetime (UTC-naive)\n",
    "            # Make UTC aware (in order to convert)\n",
    "            # Convert UTC to ET\n",
    "            # Make timezone naive\n",
    "            all_bars.index = all_bars.index.tz_localize(pytz.UTC).tz_convert(\"US/Eastern\").tz_localize(None)  \n",
    "            files.append(all_bars)\n",
    "            print(day)\n",
    "        \n",
    "    all_bars = pd.concat(files)\n",
    "    all_bars = all_bars.reset_index()\n",
    "    all_bars = all_bars.set_index('ticker')\n",
    "\n",
    "    for ticker in list(set(all_bars.index.unique())):\n",
    "        bars = all_bars.loc[ticker]\n",
    "        if isinstance(bars, pd.Series):\n",
    "            bars = all_bars.loc[[ticker]]\n",
    "        bars = bars[['datetime', 'open', 'high', 'low', 'close', 'volume']]\n",
    "        bars = bars.set_index('datetime')\n",
    "\n",
    "        if os.path.isfile(DATA_PATH + f'raw/m1/{ticker}.parquet'):\n",
    "            write(DATA_PATH + f\"raw/m1/{ticker}.parquet\", bars, append=True, compression=\"snappy\", row_group_offsets=25000)\n",
    "        else:\n",
    "            bars.to_parquet(DATA_PATH + f\"raw/m1/{ticker}.parquet\", engine=\"fastparquet\", compression=\"snappy\", row_group_offsets=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updates\n",
    "Process day-by-day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_flatfile(local_file_path):\n",
    "    \"\"\"Unzips the flat file and split or append it to ticker files.\n",
    "    \"\"\"\n",
    "    with gzip.open(local_file_path) as f:\n",
    "        all_bars = pd.read_csv(f)\n",
    "        all_bars = all_bars[['window_start', 'ticker', 'open', 'high', 'low', 'close', 'volume']]\n",
    "        all_bars = all_bars.rename(columns={'window_start': 'datetime'})\n",
    "        all_bars = all_bars.set_index('datetime')\n",
    "        all_bars.index = pd.to_datetime(all_bars.index, unit='ns') # Convert to datetime (UTC-naive)\n",
    "        all_bars.index = all_bars.index.tz_localize(pytz.UTC)  # Make UTC aware (in order to convert)\n",
    "        all_bars.index = all_bars.index.tz_convert(\"US/Eastern\")  # Convert UTC to ET\n",
    "        all_bars.index = all_bars.index.tz_localize(None)  # Make timezone naive\n",
    "        \n",
    "        for ticker in all_bars['ticker'].unique():\n",
    "            bars = all_bars[all_bars['ticker'] == ticker]\n",
    "            bars = bars[['open', 'high', 'low', 'close', 'volume']]\n",
    "\n",
    "            if os.path.isfile(DATA_PATH + f'raw/m1/{ticker}.parquet'):\n",
    "                write(DATA_PATH + f\"raw/m1/{ticker}.parquet\", bars, append=True, compression=\"snappy\", row_group_offsets=25000)\n",
    "            else:\n",
    "                bars.to_parquet(DATA_PATH + f\"raw/m1/{ticker}.parquet\", engine=\"fastparquet\", compression=\"snappy\", row_group_offsets=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in get_market_dates(START_DATE, END_DATE):\n",
    "    destination = DATA_PATH + f'raw/{day.isoformat()}.csv.gz'\n",
    "    s3.download_file('flatfiles', \n",
    "                 f'us_stocks_sip/minute_aggs_v1/{day.year}/{day.strftime(\"%m\")}/{day.isoformat()}.csv.gz', \n",
    "                 destination)\n",
    "    process_flatfile(destination)\n",
    "    os.remove(destination)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algotrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
