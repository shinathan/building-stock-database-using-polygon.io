{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Downloading raw prices\n",
    "Because we already created functions in notebook 1 to download 1-minute tickers, this is easy. We need to keep in mind that a lot of stocks such as SPACs have almost no price data. So there are a lot of empty bars. Some stocks don't even have 1 trade for the entire day. However, I cannot remove them either. Because if for one week the stock has a lot of activity, my trading systems may still trade it. That is the curse of the HTB small caps space. I cannot just filter on monthly liquidity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "from polygon.rest import RESTClient\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from utils import get_tickers, download_m1_raw_data\n",
    "import pandas as pd\n",
    "from fastparquet import write\n",
    "\n",
    "DATA_PATH = \"../data/polygon/\"\n",
    "\n",
    "with open(DATA_PATH + \"secret.txt\") as f:\n",
    "    KEY = next(f).strip()\n",
    "\n",
    "client = RESTClient(api_key=KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-parallel approach**\n",
    "\n",
    "Downloading outside of market hours is much faster.\n",
    "\n",
    "So instead of csv's we use <code>Parquet</code>. This saves a lot of disk space while sacrificing human readibility. Although you can just use [tad](https://www.tadviewer.com/) (fastparquet) or [ParquetViewer](https://github.com/mukunku/ParquetViewer/releases) (pyarrow). And pandas already supports reading from Parquet files. \n",
    "\n",
    "In <code>06_fastparquet.ipynb</code> you can see a comparison between parquet compression algorithms and csv's. We save more than 50% in disk space, while write speeds are more than x7.\n",
    "\n",
    "Downloading the data takes around 20 hours for data from 2019-01-01 to 2023-09-01. However this only has to be done once, after which you only update (append)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = get_tickers(v=3)\n",
    "tickers = tickers[tickers[\"type\"].isin([\"CS\", \"ADRC\", \"ETF\", \"ETN\", \"ETV\"])]\n",
    "tickers.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# For timing\n",
    "length = len(tickers)\n",
    "start_time = datetime.now()\n",
    "total_days_to_download = (tickers.end_date - tickers.start_date).sum()\n",
    "downloaded_days = timedelta(0)\n",
    "\n",
    "for index, row in tickers.iterrows():\n",
    "    id = row[\"ID\"]\n",
    "    ticker = row[\"ticker\"]\n",
    "    start_date = row[\"start_date\"]\n",
    "    end_date = row[\"end_date\"]\n",
    "\n",
    "    m1 = download_m1_raw_data(ticker = ticker, from_ = start_date, to = end_date, columns = [\"open\", \"high\", \"low\", \"close\", \"volume\"], client=client)\n",
    "    m1.to_parquet(DATA_PATH + f\"raw/m1/{id}.parquet\", engine=\"fastparquet\", compression=\"snappy\", row_group_offsets=25000)\n",
    "\n",
    "    # For timing (becomes accurate after 5.0%)\n",
    "    passed_time = datetime.now() - start_time\n",
    "    days_just_downloaded = end_date - start_date\n",
    "    \n",
    "    total_days_to_download -= days_just_downloaded\n",
    "    downloaded_days += days_just_downloaded\n",
    "    used_time_per_day = passed_time/downloaded_days\n",
    "\n",
    "    remaining_time = used_time_per_day*total_days_to_download\n",
    "    remaining_hours = int(remaining_time.total_seconds()/3600)\n",
    "    remaining_minutes = int((remaining_time.total_seconds()%3600)/60)\n",
    "\n",
    "    print(f\"Progress: {round(index/length*100, 1)}% | ETA: {remaining_hours} hours and {remaining_minutes} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv(DATA_PATH + f\"raw/m1/A-2021-01-01.csv\", index_col=\"datetime\")\n",
    "# pd.read_parquet(DATA_PATH + f\"raw/m1/A-2021-01-01.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parallel approach (may not work now)**\n",
    "\n",
    "The Polygon API uses the <code>requests</code> library which does not support asynchronous processing. So I have to use <code>aiohttp</code> and work with raw requests. Also we have to bother with pagination because of the 50000 limit.\n",
    "\n",
    "I used ChatGPT to convert the code above to work with aiohttp. However it did not work. After manually debugging, I got it working and the file is now in <code>06_parallel.py</code>. You can specify the maximum amount of parallel requests. Setting it to 10 should make downloading 10 times faster. Be wary to not generate too many requests.\n",
    "\n",
    "By the way, it is possible to run multiple python notebook files in VSCode. Simply copy this file multiple times, filter on <code>index</code> and run them all. This speeds it up, because the bottleneck is not processing power but the speed of a request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Updates\n",
    "Run the first cell to import the modules and then run the cells below. Making a backup of the <code>raw/m1</code> folder is recommended! If something goes wrong in this step, you cannot go back without downloading everything again. \n",
    "\n",
    "Loop through tickers_v3:\n",
    "- If the ID is in <code>tickers_v3_old</code> and <code>end_date</code> is larger than the old end date, then we can simply append the raw data.\n",
    "- If the ID is not in <code>tickers_v3</code>, we need to create a new file.\n",
    "\n",
    "All other tickers are those that have been delisted and need no updating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tickers_v3 = pd.read_csv(\"../data/tickers_v3_old.csv\", index_col = 0)\n",
    "old_tickers_v3 = old_tickers_v3[old_tickers_v3[\"type\"].isin([\"CS\", \"ADRC\", \"ETF\", \"ETN\", \"ETV\"])]\n",
    "old_tickers_v3.reset_index(inplace=True, drop=True)\n",
    "old_tickers_v3['end_date'] = pd.to_datetime(old_tickers_v3['end_date']).dt.date\n",
    "old_end_date = old_tickers_v3['end_date'].max()\n",
    "old_IDs = list(old_tickers_v3['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = get_tickers(v=3)\n",
    "tickers = tickers[tickers[\"type\"].isin([\"CS\", \"ADRC\", \"ETF\", \"ETN\", \"ETV\"])]\n",
    "tickers.reset_index(inplace=True, drop=True)\n",
    "counter = 0\n",
    "for index, row in tickers.iterrows():\n",
    "    if index < 1600:\n",
    "        continue\n",
    "    id = row[\"ID\"]\n",
    "    ticker = row[\"ticker\"]\n",
    "    start_date = row[\"start_date\"]\n",
    "    end_date = row[\"end_date\"]\n",
    "\n",
    "    if id in old_IDs and end_date > old_end_date:\n",
    "        update = download_m1_raw_data(ticker = ticker, from_ = old_end_date + timedelta(days=1), to = end_date, columns = [\"open\", \"high\", \"low\", \"close\", \"volume\"], path = None, client=client)\n",
    "        if update is not None:\n",
    "            try:\n",
    "                write(DATA_PATH + f\"raw/m1/{id}.parquet\", update, append=True)\n",
    "                print(f'Updating {id}')\n",
    "            except Exception:\n",
    "                m1 = download_m1_raw_data(ticker = ticker, from_ = row[\"start_date\"], to = end_date, columns = [\"open\", \"high\", \"low\", \"close\", \"volume\"], client=client)\n",
    "                m1.to_parquet(DATA_PATH + f\"raw/m1/{id}.parquet\", engine=\"fastparquet\", compression=\"snappy\", row_group_offsets=25000)\n",
    "                print(f'Downloading {id}')\n",
    "\n",
    "    elif id not in old_IDs:\n",
    "        download_m1_raw_data(ticker = ticker, from_ = row[\"start_date\"], to = end_date, columns = [\"open\", \"high\", \"low\", \"close\", \"volume\"], path = DATA_PATH + f\"raw/m1/{id}.parquet\", client=client)\n",
    "        print(f'Downloading {id}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algotrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
