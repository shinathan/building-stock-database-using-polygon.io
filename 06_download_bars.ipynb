{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading raw prices\n",
    "Because we already created functions in notebook 1 to download 1-minute tickers, this is easy. We need to keep in mind that a lot of stocks such as SPACs have almost no price data. So there are a lot of empty bars. Some stocks don't even have 1 trade for the entire day. However, I cannot remove them either. Because if for one week the stock has a lot of activity, my trading systems may still trade it. That is the curse of the HTB small caps space. I cannot just filter on monthly liquidity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polygon.rest import RESTClient\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from utils import get_tickers, download_m1_raw_data, datetime_to_unix\n",
    "import pytz\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATA_PATH = \"../data/polygon/\"\n",
    "\n",
    "with open(DATA_PATH + \"secret.txt\") as f:\n",
    "    KEY = next(f).strip()\n",
    "\n",
    "client = RESTClient(api_key=KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-parallel approach**\n",
    "\n",
    "Downloading outside of market hours is much faster.\n",
    "\n",
    "So instead of csv's we use <code>Parquet</code>. This saves a lot of disk space while sacrificing human readibility. Although you can just use [tad](https://www.tadviewer.com/) (fastparquet) or [ParquetViewer](https://github.com/mukunku/ParquetViewer/releases) (pyarrow). And pandas already supports reading from Parquet files. \n",
    "\n",
    "In <code>06_fastparquet.ipynb</code> you can see a comparison between parquet compression algorithms and csv's. We save more than 50% in disk space, while write speeds are more than x7.\n",
    "\n",
    "Downloading the data takes around 20 hours for data from 2019-01-01 to 2023-09-01. However this only has to be done once, after which you only update (append)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = get_tickers(v=3)\n",
    "tickers = tickers[tickers[\"type\"].isin([\"CS\", \"ADRC\", \"ETF\", \"ETN\", \"ETV\"])]\n",
    "tickers.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# For timing\n",
    "length = len(tickers)\n",
    "start_time = datetime.now()\n",
    "total_days_to_download = (tickers.end_date - tickers.start_date).sum()\n",
    "downloaded_days = timedelta(0)\n",
    "\n",
    "for index, row in tickers.iterrows():\n",
    "    if index < 4385:\n",
    "        continue\n",
    "\n",
    "    id = row[\"ID\"]\n",
    "    ticker = row[\"ticker\"]\n",
    "    start_date = row[\"start_date\"]\n",
    "    end_date = row[\"end_date\"]\n",
    "\n",
    "    download_m1_raw_data(ticker = ticker, from_ = start_date, to = end_date, columns = [\"open\", \"high\", \"low\", \"close\", \"volume\"], path = DATA_PATH + f\"raw/m1/{id}.parquet\", client=client, to_parquet=True)\n",
    "\n",
    "    # For timing (becomes accurate after 5.0%)\n",
    "    passed_time = datetime.now() - start_time\n",
    "    days_just_downloaded = end_date - start_date\n",
    "    \n",
    "    total_days_to_download -= days_just_downloaded\n",
    "    downloaded_days += days_just_downloaded\n",
    "    used_time_per_day = passed_time/downloaded_days\n",
    "\n",
    "    remaining_time = used_time_per_day*total_days_to_download\n",
    "    remaining_hours = int(remaining_time.total_seconds()/3600)\n",
    "    remaining_minutes = int((remaining_time.total_seconds()%3600)/60)\n",
    "\n",
    "    print(f\"Progress: {round(index/length*100, 1)}% | ETA: {remaining_hours} hours and {remaining_minutes} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv(DATA_PATH + f\"raw/m1/A-2021-01-01.csv\", index_col=\"datetime\")\n",
    "# pd.read_parquet(DATA_PATH + f\"raw/m1/A-2021-01-01.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parallel approach (may not work now)**\n",
    "\n",
    "The Polygon API uses the <code>requests</code> library which does not support asynchronous processing. So I have to use <code>aiohttp</code> and work with raw requests. Also we have to bother with pagination because of the 50000 limit.\n",
    "\n",
    "I used ChatGPT to convert the code above to work with aiohttp. However it did not work. After manually debugging, I got it working and the file is now in <code>06_parallel.py</code>. You can specify the maximum amount of parallel requests. Setting it to 10 should make downloading 10 times faster. Be wary to not generate too many requests.\n",
    "\n",
    "By the way, it is possible to run multiple python notebook files in VSCode. Simply copy this file multiple times, filter on <code>index</code> and run them all. This speeds it up, because the bottleneck is not processing power but the speed of a request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updates\n",
    "Not implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algotrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
