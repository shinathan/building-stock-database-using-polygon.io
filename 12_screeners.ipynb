{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.1 Simulating screeners\n",
    "In backtesting, we should be able to simulate what a screener would have given, else we would have to simulate all stocks simultaneously. Because this is a computationally expensive thing to do, the results are saved in the <code>processed/cache/</code> folder. There are of course some constraints to this. It should be technically possible to get the results from screeners, either online or via APIs (e.g. IBKR TWS). This means that intraday scanners are limited. For long-term screeners it does not matter because we can actually download everything after the end of the trading day.\n",
    "\n",
    "Warning: The screener results should not suffer from look-ahead bias. If you e.g. want to calculate the 14-day volume, you can only know this after day 14. So I always shift the trading days by 1 to account for this. Then all statistics should be available at the corresponding date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import remove_extended_hours, get_market_dates, get_tickers, get_data, first_trading_date_after_equal, first_trading_date_after\n",
    "from datetime import datetime, date, timedelta, time\n",
    "import mplfinance as mpf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import ast\n",
    "DATA_PATH = \"../data/polygon/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top liquid stocks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_top_n_liquid(n=500, start = date(2000, 1, 1), end = date(2100, 1, 1)):\n",
    "    \"\"\"Calculated the top N liquid stocks per quarter and stores them in processed/cache/top_{n}_liquid.json\n",
    "\n",
    "    Args:\n",
    "        n (int, optional): the amount of stocks. Defaults to 500.\n",
    "        start (date, optional): the start date. Defaults to date(2000, 1, 1).\n",
    "        end (date, optional): the end date. Defaults to date(2100, 1, 1).\n",
    "    \"\"\"\n",
    "    tickers = get_tickers()\n",
    "    tickers = tickers[tickers['type'] == \"CS\"]\n",
    "\n",
    "    dates_and_IDs = {} # {'2022-01-01': ['AAPL', 'MSFT', ...], '2022-04-01': ['NVDA', 'AMD', ...], ...}\n",
    "\n",
    "    quarterly_all = pd.DataFrame() # Rows are dates, columns are turnovers, column names are IDs\n",
    "    for index, id in enumerate(tickers['ID']):\n",
    "        bars = get_data(id, columns=['volume', 'close'], start=start, end=end)\n",
    "        quarterly = bars.resample('Q').agg({'close': 'last',\n",
    "                                'volume': 'sum'})\n",
    "        quarterly['turnover'] = quarterly['volume'] * quarterly['close']\n",
    "        quarterly = quarterly.rename(columns={'turnover': id}).drop(columns=['volume', 'close'])\n",
    "        quarterly_all = quarterly_all.merge(quarterly[id], how='outer', left_index=True, right_index=True)\n",
    "        #print(index)\n",
    "        \n",
    "        # Avoids defragmentation, increasing performance. Without this it would take more than 4x longer.\n",
    "        if index % 100 == 0 and index != 0:\n",
    "            quarterly_all = quarterly_all.copy()\n",
    "            print(index)\n",
    "\n",
    "    # Store results in dates_and_IDs\n",
    "    for datetime_, row in quarterly_all.copy().iterrows():\n",
    "        top_n_stocks = row[row.notna()].nlargest(n).index.tolist()\n",
    "        next_trading_date = first_trading_date_after_equal(datetime_.to_pydatetime().date() + timedelta(days=1))\n",
    "        dates_and_IDs[next_trading_date.isoformat()] = top_n_stocks\n",
    "\n",
    "    # Store to json (csv is not a convenient format to store variable length lists)\n",
    "    with open(DATA_PATH + f'processed/cache/top_{n}_liquid.json', 'w') as f: \n",
    "        json.dump(dates_and_IDs, f)\n",
    "    return\n",
    "\n",
    "def get_top_n_liquid(day, n=500):\n",
    "    \"\"\"Retrieve the top liquid N stocks. If no cache exists, calculate them.\n",
    "\n",
    "    Args:\n",
    "        day (date): the date to query\n",
    "        n (int, optional): the amount of stocks to query. Defaults to 500.\n",
    "\n",
    "    Returns:\n",
    "        list: list of IDs\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(DATA_PATH + f'processed/cache/top_{n}_liquid.json'):\n",
    "        store_top_n_liquid(n)\n",
    "        \n",
    "    with open(DATA_PATH + f'processed/cache/top_{n}_liquid.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    dates = list(data.keys())\n",
    "    date_to_query = max(list(filter(lambda x: x <= day.isoformat(), dates)) )\n",
    "    return date_to_query, data[date_to_query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-03\n",
      "['TSLA-2019-01-01', 'NVDA-2019-01-01', 'AAPL-2019-01-01', 'MSFT-2019-01-01', 'AMD-2019-01-01']\n"
     ]
    }
   ],
   "source": [
    "day, data = get_top_n_liquid(date(2023, 8, 25), n=100)\n",
    "print(day)\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top winners/losers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_top_movers(sign, percentage, time_, start = date(2000, 1, 1), end = date(2100, 1, 1)):\n",
    "    \"\"\"Store the top gainers/losers (compared to previous close) if percentage is above threshold in processed/cache/gainers_{sign}p{percentage}_t{time}.json\n",
    "    This is done for every day and specified time. \n",
    "\n",
    "    Args:\n",
    "        sign (string): either '+' or '-' for higher or lower\n",
    "        percentage (int): the percentage threshold\n",
    "        time_ (time): the time to calculate the change each day\n",
    "        start (date, optional): the start date. Defaults to date(2000, 1, 1).\n",
    "        end (date, optional): the end date. Defaults to date(2100, 1, 1).\n",
    "    \"\"\"\n",
    "    tickers = get_tickers()\n",
    "    tickers = tickers[tickers['type'].isin(['CS', 'ADRC'])]\n",
    "\n",
    "    dates = get_market_dates()\n",
    "    dates = [day for day in dates if (day >= start and day <= end)] # Filter dates\n",
    "    dates_and_IDs = {day.isoformat(): list() for day in dates} # Create dictionary with empty lists\n",
    "\n",
    "    for index, row in tickers.iterrows():\n",
    "        id = row['ID']\n",
    "        daily = get_data(id, columns=['close'], start=start, end=end)\n",
    "        daily['prev_close'] = daily['close'].shift(1)\n",
    "        minute = get_data(id, timeframe=1, columns=['close'], start=start, end=end)\n",
    "        minute.rename(columns={'close': 'close_at_time'}, inplace=True)\n",
    "\n",
    "        # If there is no data between the start and end date, skip the ticker\n",
    "        if daily.empty or minute.empty:\n",
    "            continue\n",
    "\n",
    "        # TO REMOVE\n",
    "        minute = minute[~minute.index.duplicated()]\n",
    "        daily = daily[~daily.index.duplicated()]\n",
    "\n",
    "        # Build our data for this ticker\n",
    "        data = minute.between_time(time_, time_)\n",
    "        data.index = pd.DatetimeIndex(data.index).normalize() # Remove the 'time' part, which mean setting at 00:00 in order to merge with daily data\n",
    "\n",
    "        data = pd.concat([data, daily[['prev_close']]], axis=1)\n",
    "        data['change'] = 100*(data['close_at_time'] / data['prev_close'] - 1)\n",
    "\n",
    "        # Select the dates that had a change higher/lower than the threshold\n",
    "        if sign == '+':\n",
    "            mover_dates = data[data.change > percentage].index\n",
    "        elif sign == '-':\n",
    "            mover_dates = data[data.change < percentage].index\n",
    "\n",
    "        mover_dates = pd.to_datetime(mover_dates).date\n",
    "\n",
    "        # Put it in the dictionary\n",
    "        for day in mover_dates:\n",
    "            dates_and_IDs[day.isoformat()].append(id)\n",
    "        \n",
    "        # For timing\n",
    "        if index % 250 == 0:\n",
    "            print(index)\n",
    "\n",
    "    # Store to json\n",
    "    with open(DATA_PATH + f'processed/cache/gainers_p{percentage}{sign}_t{time_.strftime(\"%H%M\")}.json', 'w') as f: \n",
    "        json.dump(dates_and_IDs, f)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "store_top_movers(sign = '+', percentage = 20, time_ = time(16, 29), start = date(2019, 1, 1), end = date(2023, 9, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_top_movers(sign = '-', percentage = 20, time_ = time(16, 29), start = date(2019, 1, 1), end = date(2023, 9, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should always keep original ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = date(2019, 1, 1)\n",
    "end = date(2023, 9, 1)\n",
    "\n",
    "id = 'PRDO-2019-01-01'\n",
    "daily = get_data(id, columns=['close'], start=start, end=end)\n",
    "daily['prev_close'] = daily['close'].shift(1)\n",
    "minute = get_data(id, timeframe=1, columns=['close'], start=start, end=end)\n",
    "minute.rename(columns={'close': 'close_at_time'}, inplace=True)\n",
    "\n",
    "data = minute.between_time(time(17, 59), time(17, 59))\n",
    "data.index = pd.DatetimeIndex(data.index).normalize() # Remove the 'time' part, which mean setting at 00:00 in order to merge with daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.index.duplicated()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algotrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
