{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.1 . Simulating screeners\n",
    "In backtesting, we should be able to simulate what a screener would have given, else we would have to simulate all stocks simultaneously. Because this is a computationally expensive thing to do, the results are saved in the <code>processed/cache/</code> folder. There are of course some constraints to this. It should be technically possible to get the results from screeners, either online or via APIs (e.g. IBKR TWS). This means that intraday scanners are limited. For long-term screeners it does not matter because we can actually download everything after the end of the trading day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import remove_extended_hours, get_market_dates, get_tickers, get_data, first_trading_date_after_equal\n",
    "from datetime import datetime, date, timedelta\n",
    "import mplfinance as mpf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import ast\n",
    "DATA_PATH = \"../data/polygon/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_top_n_liquid(n=500, start = date(2000, 1, 1), end = date(2100, 1, 1)):\n",
    "    tickers = get_tickers()\n",
    "    tickers = tickers[tickers['type'] == \"CS\"]\n",
    "\n",
    "    dates_and_IDs = {} # {'2022-01-01': ['AAPL', 'MSFT', ...], '2022-04-01': ['NVDA', 'AMD', ...], ...}\n",
    "\n",
    "    quarterly_all = pd.DataFrame()\n",
    "    for index, id in enumerate(tickers['ID']):\n",
    "        bars = get_data(id, columns=['volume', 'close'], start=start, end=end)\n",
    "        quarterly = bars.resample('Q').agg({'close': 'last',\n",
    "                                'volume': 'sum'})\n",
    "        quarterly['turnover'] = quarterly['volume'] * quarterly['close']\n",
    "        quarterly = quarterly.rename(columns={'turnover': id}).drop(columns=['volume', 'close'])\n",
    "        quarterly_all = quarterly_all.merge(quarterly[id], how='outer', left_index=True, right_index=True)\n",
    "        #print(index)\n",
    "        \n",
    "        # Avoids defragmentation, increasing performance. Without this it would take more than 4x longer.\n",
    "        if index % 100 == 0 and index != 0:\n",
    "            quarterly_all = quarterly_all.copy()\n",
    "            print(index)\n",
    "\n",
    "    # Store results in dates_and_IDs\n",
    "    for datetime_, row in quarterly_all.copy().iterrows():\n",
    "        top_n_stocks = row[row.notna()].nlargest(n).index.tolist()\n",
    "        next_trading_date = first_trading_date_after_equal(datetime_.to_pydatetime().date() + timedelta(days=1))\n",
    "        dates_and_IDs[next_trading_date.isoformat()] = top_n_stocks\n",
    "\n",
    "    # Store to json (csv is not a convenient format to store variable length lists)\n",
    "    with open(DATA_PATH + f'processed/cache/top_{n}_liquid.json', 'w') as f: \n",
    "        json.dump(dates_and_IDs, f)\n",
    "    return\n",
    "\n",
    "def get_top_n_liquid(day, n=500):\n",
    "    if not os.path.isfile(DATA_PATH + f'processed/cache/top_{n}_liquid.json'):\n",
    "        store_top_n_liquid(n)\n",
    "        \n",
    "    with open(DATA_PATH + f'processed/cache/top_{n}_liquid.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    dates = list(data.keys())\n",
    "    date_to_query = max(list(filter(lambda x: x <= day.isoformat(), dates)) )\n",
    "    return date_to_query, data[date_to_query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-03\n",
      "['TSLA-2019-01-01', 'NVDA-2019-01-01', 'AAPL-2019-01-01', 'MSFT-2019-01-01', 'AMD-2019-01-01']\n"
     ]
    }
   ],
   "source": [
    "date_, data = get_top_n_liquid(date(2023, 8, 25), n=100)\n",
    "print(date_)\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top gainers %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Testing a mean-reversion strategy on SPY.\n",
    "I will try out a popular strategy using the IBS indicator. I want to see if I can match the results on [here](https://www.quantifiedstrategies.com/internal-bar-strength-ibs-indicator-strategy/). I will also use pure pandas.\n",
    "\n",
    "* Universe: SPY ETF\n",
    "* Entry: IBS < 0.2\n",
    "* Exit: IBS > 0.8\n",
    "* Trading on close prices.\n",
    "\n",
    "Although it is possible to get the exact closing price using market-on-close orders, you cannot know the value of the IBS at market close. So as an extension I will look at how the strategy has performed if the IBS is calculated using daily bars exluding the last minute.\n",
    "\n",
    "I also want to look at the impact of taxes (36%) and inflation.\n",
    "\n",
    "Note: I would not use the SPY for trading indices, because futures are more liquid. Even index CFDs may have lower trading costs. Also, I can only access US-domiciled ETFs from a very small amount of brokers due to stupid EU regulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algotrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
