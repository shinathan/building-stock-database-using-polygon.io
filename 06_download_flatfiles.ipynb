{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download flat files\n",
    "This was only possible from April 2024. We simply download everything and extract them to single ticker files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytz\n",
    "import gzip\n",
    "import calendar\n",
    "import pandas as pd\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from pytz import timezone\n",
    "from times import get_market_dates\n",
    "from fastparquet import write\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "DATA_PATH = \"../data/polygon/\"\n",
    "\n",
    "START_DATE = date(2003, 9, 10)\n",
    "END_DATE = date(2024, 4, 19)\n",
    "\n",
    "session = boto3.Session(\n",
    "   aws_access_key_id='7203c471-037b-4944-96b0-effc0d3911b3',\n",
    "   aws_secret_access_key='IOOFCMHAT7plpPitNmqFICLdG1AnhC5l',\n",
    ")\n",
    "s3 = session.client(\n",
    "   's3',\n",
    "   endpoint_url='https://files.polygon.io',\n",
    "   config=Config(signature_version='s3v4'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial download\n",
    "Download everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in get_market_dates(date(2003, 9, 10), END_DATE):\n",
    "    destination = DATA_PATH + f'raw/flatfiles/{day.isoformat()}.csv.gz'\n",
    "    s3.download_file('flatfiles', \n",
    "                f'us_stocks_sip/minute_aggs_v1/{day.year}/{day.strftime(\"%m\")}/{day.isoformat()}.csv.gz', \n",
    "                destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and concatenate into monthly files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_and_save(start_date, end_date, file_name):\n",
    "    files = []\n",
    "    market_dates = get_market_dates(start_date, end_date)\n",
    "    if len(market_dates) == 0:\n",
    "        return\n",
    "    for day in market_dates:\n",
    "        destination = DATA_PATH + f'raw/flatfiles/{day.isoformat()}.csv.gz'\n",
    "        with gzip.open(destination) as f:\n",
    "            all_bars = pd.read_csv(f)\n",
    "            all_bars = all_bars[['window_start', 'ticker', 'open', 'high', 'low', 'close', 'volume']]\n",
    "            all_bars = all_bars.rename(columns={'window_start': 'datetime'})\n",
    "            all_bars = all_bars.set_index('datetime')\n",
    "            all_bars.index = pd.to_datetime(all_bars.index, unit='ns') # Convert to datetime (UTC-naive)\n",
    "            # Make UTC aware (in order to convert)\n",
    "            # Convert UTC to ET\n",
    "            # Make timezone naive\n",
    "            all_bars.index = all_bars.index.tz_localize(pytz.UTC).tz_convert(\"US/Eastern\").tz_localize(None)  \n",
    "\n",
    "            files.append(all_bars)\n",
    "            print(day)\n",
    "        \n",
    "    all_bars = pd.concat(files)\n",
    "    all_bars = all_bars.reset_index()\n",
    "    all_bars = all_bars.set_index('ticker')\n",
    "    all_bars = all_bars.sort_index()\n",
    "\n",
    "    all_bars.to_parquet(DATA_PATH + f\"raw/flatfiles/{file_name}.parquet\", engine=\"fastparquet\", compression=\"snappy\", row_group_offsets=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2003, 2024 + 1):\n",
    "    files = []\n",
    "    for month in range(1, 12 + 1):\n",
    "        _, end_date = calendar.monthrange(year, month)\n",
    "        concatenate_and_save(date(year, month, 1), date(year, month, end_date), f\"{year}-{month}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the monthly files which contains all ticker into individual ticker files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(2003, 2024+1):\n",
    "    files = []\n",
    "    for month in range(1, 12+1):\n",
    "        print(f'{datetime.now()} | {year}-{month}')\n",
    "        if not os.path.isfile(DATA_PATH + f\"raw/flatfiles/{year}-{month}.parquet\"):\n",
    "            continue\n",
    "        \n",
    "        all_bars = pd.read_parquet(DATA_PATH + f\"raw/flatfiles/{year}-{month}.parquet\")\n",
    "        all_bars = all_bars[~all_bars.index.isna()]\n",
    "\n",
    "        if all_bars['datetime'].min().year != year or all_bars['datetime'].min().month != month:\n",
    "            raise Exception(f'{year} | {month} HAS OUT OF BOUND DATES')\n",
    "\n",
    "        # File names are case insensitive! This lead to big data errors (e.g. TpC and TPC are merged)\n",
    "        # So we simply remove all tickers that have small letters.\n",
    "        # Which we don't need anyways, because small letter = non-common stock.\n",
    "        for ticker in list(filter(lambda ticker: not any(s.islower() for s in ticker), list(all_bars.index.unique()))):\n",
    "            bars = all_bars.loc[ticker]\n",
    "            if isinstance(bars, pd.Series):\n",
    "                bars = all_bars.loc[[ticker]]\n",
    "            bars = bars[['datetime', 'open', 'high', 'low', 'close', 'volume']]\n",
    "            bars = bars.set_index('datetime').sort_index()\n",
    "            \n",
    "            # Windows quirk note: you cannot save files called 'prn'. Of course there is a ticker that is named PRN...\n",
    "            # So we name it 'PRN_'. I hope there are no tickers named NULL or something please.\n",
    "            if ticker == 'PRN':\n",
    "                ticker = 'PRN_'\n",
    "\n",
    "            if os.path.isfile(DATA_PATH + f'raw/m1/{ticker}.parquet'):\n",
    "                write(DATA_PATH + f\"raw/m1/{ticker}.parquet\", bars, append=True, compression=\"snappy\", row_group_offsets=25000)\n",
    "            else:\n",
    "                bars.to_parquet(DATA_PATH + f\"raw/m1/{ticker}.parquet\", engine=\"fastparquet\", compression=\"snappy\", row_group_offsets=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The old version of the above code runs very fast, up to end 2022. Then it becomes 20-50x slower. I found the problem:\n",
    "\n",
    "Pandas dataframe lookup uses hash-tables so the time complexity is O(1). However, if you have null values in the index this does not apply anymore! The bars from end 2022 and upwards have null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updates\n",
    "Process day-by-day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_flatfile(local_file_path):\n",
    "    \"\"\"Unzips the flat file and split or append it to ticker files.\n",
    "    \"\"\"\n",
    "    with gzip.open(local_file_path) as f:\n",
    "        all_bars = pd.read_csv(f)\n",
    "        all_bars = all_bars[['window_start', 'ticker', 'open', 'high', 'low', 'close', 'volume']]\n",
    "        all_bars = all_bars.rename(columns={'window_start': 'datetime'})\n",
    "        all_bars = all_bars.set_index('datetime')\n",
    "        all_bars.index = pd.to_datetime(all_bars.index, unit='ns') # Convert to datetime (UTC-naive)\n",
    "        all_bars.index = all_bars.index.tz_localize(pytz.UTC)  # Make UTC aware (in order to convert)\n",
    "        all_bars.index = all_bars.index.tz_convert(\"US/Eastern\")  # Convert UTC to ET\n",
    "        all_bars.index = all_bars.index.tz_localize(None)  # Make timezone naive\n",
    "        \n",
    "        for ticker in all_bars['ticker'].unique():\n",
    "            bars = all_bars[all_bars['ticker'] == ticker]\n",
    "            bars = bars[['open', 'high', 'low', 'close', 'volume']]\n",
    "\n",
    "            if os.path.isfile(DATA_PATH + f'raw/m1/{ticker}.parquet'):\n",
    "                write(DATA_PATH + f\"raw/m1/{ticker}.parquet\", bars, append=True, compression=\"snappy\", row_group_offsets=25000)\n",
    "            else:\n",
    "                bars.to_parquet(DATA_PATH + f\"raw/m1/{ticker}.parquet\", engine=\"fastparquet\", compression=\"snappy\", row_group_offsets=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in get_market_dates(START_DATE, END_DATE):\n",
    "    destination = DATA_PATH + f'raw/{day.isoformat()}.csv.gz'\n",
    "    s3.download_file('flatfiles', \n",
    "                 f'us_stocks_sip/minute_aggs_v1/{day.year}/{day.strftime(\"%m\")}/{day.isoformat()}.csv.gz', \n",
    "                 destination)\n",
    "    process_flatfile(destination)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algotrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
