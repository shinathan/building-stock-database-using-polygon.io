{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Simulating screeners\n",
    "In backtesting, we should be able to simulate what a screener would have given, else we would have to simulate all stocks simultaneously. Because this is a computationally expensive thing to do, the results are saved in the <code>output/screens</code> folder. There are of course some constraints to this. It should be technically possible to get the results from screeners, either online or via APIs (e.g. IBKR TWS). This means that intraday scanners are limited. For long-term screeners it does not matter because we can actually download everything after the end of the trading day.\n",
    "\n",
    "Warning: The screener results should not suffer from look-ahead bias. If you e.g. want to calculate the 14-day volume, you can only know this after day 14. So I always shift the trading days by 1 to account for this. Then all statistics should be available at the corresponding date.\n",
    "\n",
    "The screeners produce watchlists. There are two types:\n",
    "* Daily watchlists. These have a value for every day and the key is a date.\n",
    "* Intraday watchlists. The keys are datetimes, however there may not be a ticker list for every minute (to save time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import remove_extended_hours, get_market_dates, get_market_calendar, get_tickers, get_data, \\\n",
    "    first_trading_date_after_equal, first_trading_date_after, last_trading_date_before, last_trading_date_before_equal\n",
    "from datetime import datetime, date, timedelta, time\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import mplfinance as mpf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import ast\n",
    "DATA_PATH = \"../data/polygon/\"\n",
    "\n",
    "START_DATE = date(2019, 6, 1)\n",
    "END_DATE = date(2024, 3, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top liquid stocks**\n",
    "\n",
    "In backtesting, instead of screening all stocks, for most systems I only need e.g. the S&P500 stocks. However I don't care about the exact holdings of the S&P500. The only reason why I would choose the index is because of volume and liquidity. However then turnover is way more informative.\n",
    "\n",
    "So to create the T100, T500, T1500 and T3000 indices, I simply select the stocks with the highest turnover in each 6-month period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = get_tickers()\n",
    "tickers = tickers[tickers['type'] == 'CS'] # No ETFs, ADRs and indices\n",
    "dates_and_IDs = {} # {'2022-01-01': ['AAPL', 'MSFT', ...], '2022-07-01': ['NVDA', 'AMD', ...], ...}\n",
    "\n",
    "# Loop through half years\n",
    "for end_datetime in pd.date_range(start= START_DATE ,end = END_DATE,freq='6M', inclusive='right'):\n",
    "    start_date = end_datetime.date() - relativedelta(months=6)\n",
    "    # Get trading dates\n",
    "    end_date = last_trading_date_before_equal(end_datetime.date())\n",
    "    end_date_to_query = last_trading_date_before(end_date) # Subtract 1 day to account for look-ahead bias\n",
    "    start_date = first_trading_date_after_equal(start_date)\n",
    "\n",
    "    all_bars = []\n",
    "    # Find IDs with data in the specific half year that are not delisted before end of year\n",
    "    for i, row in tickers[tickers['end_date'] >= end_date_to_query].iterrows():\n",
    "        bars = get_data(row['ID'], start_date, end_date_to_query, columns=['close', 'volume'])\n",
    "        bars['ID'] = row['ID']\n",
    "        all_bars.append(bars)\n",
    "        \n",
    "    # Calculate turnover\n",
    "    all_tickers = pd.concat(all_bars)\n",
    "    all_tickers = all_tickers.groupby(\"ID\").agg({\"volume\": \"sum\", \"close\": \"mean\"})\n",
    "    all_tickers['turnover'] = all_tickers['close'] * all_tickers['volume']\n",
    "    all_tickers = all_tickers[['turnover']]\n",
    "    all_tickers.sort_values(by=\"turnover\", ascending=False, inplace=True)\n",
    "    dates_and_IDs[end_date.isoformat()] = list(all_tickers.head(500).index)\n",
    "    print(end_datetime)\n",
    "\n",
    "# Store to json\n",
    "with open('../data/output/dailyscreens/T500.json', 'w') as f: \n",
    "    json.dump(dates_and_IDs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AMZN-2019-06-03',\n",
       " 'AAPL-2019-06-03',\n",
       " 'META-2022-06-09',\n",
       " 'MSFT-2019-06-03',\n",
       " 'AMD-2019-06-03']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/output/dailyscreens/T500.json', 'r') as f: \n",
    "    T500 = json.load(f)\n",
    "\n",
    "T500[date(2019, 6, 28).isoformat()][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top winners/losers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_top_movers(sign, percentage, time_, start = date(2000, 1, 1), end = date(2100, 1, 1)):\n",
    "    \"\"\"Store the top gainers/losers (compared to previous close) if percentage is above threshold in processed/cache/gainers_{sign}p{percentage}_t{time}.json\n",
    "    This is done for every day and specified time. \n",
    "\n",
    "    Args:\n",
    "        sign (string): either '+' or '-' for winners or losers\n",
    "        percentage (float/int): the percentage threshold\n",
    "        time_ (time): the time to calculate the change each day\n",
    "        start (date, optional): the start date. Defaults to date(2000, 1, 1).\n",
    "        end (date, optional): the end date. Defaults to date(2100, 1, 1).\n",
    "    \"\"\"\n",
    "    tickers = get_tickers()\n",
    "    tickers = tickers[tickers['type'].isin(['CS', 'ADRC'])]\n",
    "\n",
    "    dates = get_market_dates()\n",
    "    dates = [day for day in dates if (day >= start and day <= end)] # Filter dates\n",
    "    dates_and_IDs = {day.isoformat(): list() for day in dates} # Create dictionary with empty lists\n",
    "\n",
    "    for index, row in tickers.iterrows():\n",
    "        id = row['ID']\n",
    "        daily = get_data(id, columns=['close'], start=start, end=end)\n",
    "        daily['prev_close'] = daily['close'].shift(1)\n",
    "        minute = get_data(id, timeframe=1, columns=['close'], start=start, end=end)\n",
    "        minute.rename(columns={'close': 'close_at_time'}, inplace=True)\n",
    "\n",
    "        # If there is no data between the start and end date, skip the ticker\n",
    "        if daily.empty or minute.empty:\n",
    "            continue\n",
    "\n",
    "        # TO REMOVE\n",
    "        minute = minute[~minute.index.duplicated()]\n",
    "        daily = daily[~daily.index.duplicated()]\n",
    "\n",
    "        # Build our data for this ticker\n",
    "        data = minute.between_time(time_, time_)\n",
    "        data.index = pd.DatetimeIndex(data.index).normalize() # Remove the 'time' part, which mean setting at 00:00 in order to merge with daily data\n",
    "\n",
    "        data = pd.concat([data, daily[['prev_close']]], axis=1)\n",
    "        data['change'] = 100*(data['close_at_time'] / data['prev_close'] - 1)\n",
    "\n",
    "        # Select the dates that had a change higher/lower than the threshold\n",
    "        if sign == '+':\n",
    "            mover_dates = data[data.change > percentage].index\n",
    "        elif sign == '-':\n",
    "            mover_dates = data[data.change < percentage].index\n",
    "\n",
    "        mover_dates = pd.to_datetime(mover_dates).date\n",
    "\n",
    "        # Put it in the dictionary\n",
    "        for day in mover_dates:\n",
    "            dates_and_IDs[day.isoformat()].append(id)\n",
    "        \n",
    "        # For timing\n",
    "        if index % 250 == 0:\n",
    "            print(index)\n",
    "\n",
    "    # Store to json\n",
    "    with open(DATA_PATH + f'processed/cache/movers_p{percentage}{sign}_t{time_.strftime(\"%H%M\")}.json', 'w') as f: \n",
    "        json.dump(dates_and_IDs, f)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_top_movers(sign = '+', percentage = 20, time_ = time(13, 00), start = date(2019, 1, 1), end = date(2023, 9, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_top_intraday_movers(sign, percentage, time_, start = date(2000, 1, 1), end = date(2100, 1, 1)):\n",
    "    \"\"\"Store the top intraday gainers/losers (compared to daily open) if percentage is above threshold in processed/cache/gainers_{sign}p{percentage}_t{time}.json\n",
    "    This is done for every day and specified time. \n",
    "\n",
    "    Args:\n",
    "        sign (string): either '+' or '-' for winners or losers\n",
    "        percentage (float/int): the percentage threshold\n",
    "        time_ (time): the time to calculate the change each day\n",
    "        start (date, optional): the start date. Defaults to date(2000, 1, 1).\n",
    "        end (date, optional): the end date. Defaults to date(2100, 1, 1).\n",
    "    \"\"\"\n",
    "    tickers = get_tickers()\n",
    "    tickers = tickers[tickers['type'].isin(['CS', 'ADRC'])]\n",
    "\n",
    "    dates = get_market_dates()\n",
    "    dates = [day for day in dates if (day >= start and day <= end)] # Filter dates\n",
    "    dates_and_IDs = {day.isoformat(): list() for day in dates} # Create dictionary with empty lists\n",
    "\n",
    "    for index, row in tickers.iterrows():\n",
    "        id = row['ID']\n",
    "        daily = get_data(id, columns=['open'], start=start, end=end)\n",
    "        minute = get_data(id, timeframe=1, columns=['close'], start=start, end=end)\n",
    "        minute.rename(columns={'close': 'close_at_time'}, inplace=True)\n",
    "\n",
    "        # If there is no data between the start and end date, skip the ticker\n",
    "        if daily.empty or minute.empty:\n",
    "            continue\n",
    "\n",
    "        # # TO REMOVE\n",
    "        # minute = minute[~minute.index.duplicated()]\n",
    "        # daily = daily[~daily.index.duplicated()]\n",
    "\n",
    "        # Build our data for this ticker\n",
    "        data = minute.between_time(time_, time_)\n",
    "        data.index = pd.DatetimeIndex(data.index).normalize() # Remove the 'time' part, which mean setting at 00:00 in order to merge with daily data\n",
    "\n",
    "        data = pd.concat([data, daily[['open']]], axis=1)\n",
    "        data['change'] = 100*(data['close_at_time'] / data['open'] - 1)\n",
    "\n",
    "        # Select the dates that had a change higher/lower than the threshold\n",
    "        if sign == '+':\n",
    "            mover_dates = data[data.change > percentage].index\n",
    "        elif sign == '-':\n",
    "            mover_dates = data[data.change < percentage].index\n",
    "\n",
    "        mover_dates = pd.to_datetime(mover_dates).date\n",
    "\n",
    "        # Put it in the dictionary\n",
    "        for day in mover_dates:\n",
    "            dates_and_IDs[day.isoformat()].append(id)\n",
    "        \n",
    "        # For timing\n",
    "        if index % 250 == 0:\n",
    "            print(index)\n",
    "\n",
    "    # Store to json\n",
    "    with open(DATA_PATH + f'processed/cache/movers_intraday_p{percentage}{sign}_t{time_.strftime(\"%H%M\")}.json', 'w') as f: \n",
    "        json.dump(dates_and_IDs, f)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_top_intraday_movers(sign = '+', percentage = 20, time_ = time(15, 55), start = date(2019, 1, 1), end = date(2023, 9, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algotrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
