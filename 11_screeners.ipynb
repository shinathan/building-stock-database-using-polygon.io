{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Simulating screeners\n",
    "In backtesting, sometimes we want to backtest all stocks. However this is computationally expensive thing to do. A better thing to do, is to to a preliminary screens and save them in the <code>output/screens</code> folder. Then when backtesting, we only use the tickers from the screens. This also resembles live trading, where the output from the screeners is used instead of downloading 5000+ symbols and scanning them.\n",
    "\n",
    "Warning: The screener results should not suffer from look-ahead bias. If you e.g. want to calculate the 14-day volume on day 1, you only have this available at the end of day 1. To account for this, sometimes you need to subtract 1 day when doing the calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import get_data\n",
    "from tickers import get_tickers, get_id\n",
    "from times import  first_trading_date_after_equal, first_trading_date_after, last_trading_date_before, \\\n",
    "    last_trading_date_before_equal, get_market_calendar, get_market_dates\n",
    "\n",
    "from datetime import datetime, date, timedelta, time\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import mplfinance as mpf\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "DATA_PATH = \"../data/polygon/\"\n",
    "\n",
    "START_DATE = date(2003, 10, 1)\n",
    "END_DATE = date(2024, 4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top liquid stocks**\n",
    "\n",
    "(I now use fundamentals instead)\n",
    "\n",
    "In backtesting, instead of screening all stocks, for most systems I only need e.g. the S&P500 stocks. However I don't care about the exact holdings of the S&P500. The only reason why I would choose the index is because of volume and liquidity. However then turnover is way more informative.\n",
    "\n",
    "So to create the T100, T500, T1500 and T3000 indices, I simply select the stocks with the highest turnover in each 6-month period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tickers = get_tickers()\n",
    "# tickers = tickers[tickers['type'] == 'CS'] # No ETFs, ADRs and indices\n",
    "# dates_and_IDs = {} # {'2022-01-01': ['AAPL', 'MSFT', ...], '2022-07-01': ['NVDA', 'AMD', ...], ...}\n",
    "\n",
    "# # Loop through half years\n",
    "# for end_datetime in pd.date_range(start= START_DATE ,end = END_DATE,freq='6M', inclusive='right'):\n",
    "#     start_date = end_datetime.date() - relativedelta(months=6)\n",
    "#     # Get trading dates\n",
    "#     end_date = last_trading_date_before_equal(end_datetime.date())\n",
    "#     end_date_to_query = last_trading_date_before(end_date) # Subtract 1 day to account for look-ahead bias\n",
    "#     start_date = first_trading_date_after_equal(start_date)\n",
    "\n",
    "#     all_bars = []\n",
    "#     # Find IDs with data in the specific half year that are not delisted before end of year\n",
    "#     for i, row in tickers[tickers['end_date'] >= end_date_to_query].iterrows():\n",
    "#         bars = get_data(row['ID'], start_date, end_date_to_query, columns=['close', 'turnover'])\n",
    "#         bars['ID'] = row['ID']\n",
    "#         all_bars.append(bars)\n",
    "        \n",
    "#     # Calculate turnover\n",
    "#     all_tickers = pd.concat(all_bars)\n",
    "#     all_tickers = all_tickers.groupby(\"ID\").agg({\"turnover\": \"sum\", \"close\": \"mean\"})\n",
    "#     all_tickers.sort_values(by=\"turnover\", ascending=False, inplace=True)\n",
    "#     dates_and_IDs[end_date.isoformat()] = list(all_tickers.head(500).index)\n",
    "    \n",
    "#     print(end_datetime)\n",
    "\n",
    "# # Store to json\n",
    "# with open('../output/screens/T500.json', 'w') as f: \n",
    "#     json.dump(dates_and_IDs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../output/screens/T500.json', 'r') as f: \n",
    "#     T500 = json.load(f)\n",
    "\n",
    "# T500[date(2019, 6, 28).isoformat()][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More than 20% intraday droppers**\n",
    "* Common stocks only\n",
    "* Turnover for the day has to be at least $5M\n",
    "* Original price has to be above $1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = get_tickers()\n",
    "tickers = tickers[tickers['type'].isin(['CS'])]\n",
    "\n",
    "# Get list of market dates within our range\n",
    "dates = get_market_dates(START_DATE, END_DATE)\n",
    "dates_and_IDs = {day.isoformat(): list() for day in dates} # Create dictionary with empty lists\n",
    "\n",
    "# Get market calendar in order to handle early closes\n",
    "market_calendar = get_market_calendar(format='datetime')[['regular_close']]\n",
    "\n",
    "for index, row in tickers.iterrows():\n",
    "    id = row['ID']\n",
    "    \n",
    "    m1 = get_data(id, START_DATE, END_DATE, timeframe=5, extended_hours=False, \\\n",
    "                  columns=['open', 'close', 'turnover', 'close_original'])\n",
    "\n",
    "    if m1.empty:\n",
    "        continue\n",
    "\n",
    "    m1_open = m1[['open']]\n",
    "    m1_close = m1[['close', 'close_original']]\n",
    "    m1_turnover = m1[['close', 'turnover']]\n",
    "    \n",
    "    # For every day, get the open price at 9:30 and close price 5 minutes before market close\n",
    "    m1_open = m1_open.between_time('9:30', '9:30')\n",
    "    m1_open.index = m1_open.index.date\n",
    "    \n",
    "    m1_close = m1_close.reindex(market_calendar['regular_close'] - timedelta(minutes=4)).dropna()\n",
    "    m1_close.index = m1_close.index.date\n",
    "\n",
    "    # Get the turnover (ignoring early closes...)\n",
    "    m1_turnover = m1_turnover.between_time('9:30', '15:55')\n",
    "    m1_turnover.index = m1_turnover.index.date\n",
    "    m1_turnover = m1_turnover.groupby(m1_turnover.index).agg({\"turnover\": \"sum\"})\n",
    "    m1_turnover = m1_turnover[['turnover']]\n",
    "\n",
    "    # Calculate the drop and append to dates_and_IDs\n",
    "    m1_open_close = pd.concat([m1_open, m1_close, m1_turnover], axis='columns')\n",
    "    m1_open_close['change'] = (m1_open_close['close']/m1_open_close['open'] - 1)*100 # Base 100\n",
    "    big_drop = m1_open_close[(m1_open_close['change'] < -20) \\\n",
    "                           & (m1_open_close['close_original'] >= 1.00)\\\n",
    "                           & (m1_open_close['turnover'] >= 5_000_000)]\n",
    "\n",
    "    for day in big_drop.index:\n",
    "        dates_and_IDs[day.isoformat()].append(id)\n",
    "        \n",
    "    if index % 50 == 0:\n",
    "        print(index)\n",
    "        \n",
    "# Store to json\n",
    "with open('../output/screens/INTRADAY_MINUS_20PCT.json', 'w') as f: \n",
    "    json.dump(dates_and_IDs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top gappers at market open**\n",
    "* Gap at least 30% (previous day regular close until 9:30 AM (9:30 AM is the close of the bar at 9:25 for 5-min and 9:29 for 1-min))\n",
    "* Pre-market turnover at least $1M\n",
    "* Unadjusted price at least $1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = get_tickers()\n",
    "tickers = tickers[tickers['type'].isin(['CS', 'ADRC'])]\n",
    "\n",
    "# Get list of market dates within our range\n",
    "dates = get_market_dates(START_DATE, END_DATE)\n",
    "dates_and_IDs = {day.isoformat(): list() for day in dates} # Create dictionary with empty lists\n",
    "\n",
    "for index, row in tickers.iterrows():\n",
    "    id = row['ID']\n",
    "\n",
    "    m1 = get_data(id, START_DATE, END_DATE, timeframe=5, extended_hours=True, \\\n",
    "                  columns=['close', 'turnover', 'close_original'])\n",
    "    d1 = get_data(id, START_DATE, END_DATE, timeframe='daily', extended_hours=False, \\\n",
    "                  columns=['close'])\n",
    "\n",
    "    if m1.empty:\n",
    "        continue\n",
    "\n",
    "    # To get the previous close\n",
    "    d1['prev_close'] = d1['close'].shift(1)\n",
    "    d1.index = d1.index.date\n",
    "\n",
    "    # To get current open and pre-market volume\n",
    "    m1 = m1.between_time('4:00', '9:29')\n",
    "    m1.index = m1.index.date\n",
    "    d1_premkt = m1.groupby(m1.index).agg({\"close\": \"last\", # This close is the 9:30 time\n",
    "                                   \"close_original\": \"last\", \n",
    "                                   \"turnover\": \"sum\"})\n",
    "\n",
    "    # Calculate gap\n",
    "    gaps = d1_premkt.merge(d1[['prev_close']], left_index=True, right_index=True)\n",
    "    gaps['gap'] = (gaps['close']/gaps['prev_close'] - 1)*100 # Base 100\n",
    "\n",
    "    big_gap = gaps[(gaps['gap'] >= 30) \\\n",
    "               & (gaps['close_original'] >= 1.00)\\\n",
    "               & (gaps['turnover'] >= 1e6)]\n",
    "    \n",
    "    for day in big_gap.index:\n",
    "        dates_and_IDs[day.isoformat()].append(id)\n",
    "        \n",
    "    if index % 50 == 0:\n",
    "        print(index)\n",
    "\n",
    "# # Store to json\n",
    "with open('../output/screens/GAP_30PCT.json', 'w') as f: \n",
    "    json.dump(dates_and_IDs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algotrading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
